{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\srujanat\\\\Desktop\\\\CS109\\\\dog-project-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x419bcfc4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "99% of human faces were detected and 12% of dog faces were detected as human face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "\n",
    "def performance(files):\n",
    "    tot=0\n",
    "    for i in files:\n",
    "        if face_detector(i):\n",
    "            tot +=1\n",
    "    return 100*(tot/len(files))\n",
    "\n",
    "print (performance(human_files_short))\n",
    "print(performance(dog_files_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__ Majority of the time we will not be able to have images with clear face. So I don't think it is reasonable expectation. For images that are not clear we can train the model with augmented data where same image is rotated in different directions/angles.\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ \n",
    "1.0% of human images are detected as dog faces and 100.0% of dog images are detected as dog faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "\n",
    "def performance(files):\n",
    "    tot=0\n",
    "    for i in files:\n",
    "        if dog_detector(i):\n",
    "            tot +=1\n",
    "    return 100*(tot/len(files))\n",
    "\n",
    "print (performance(human_files_short))\n",
    "print(performance(dog_files_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6680/6680 [03:47<00:00, 29.40it/s]\n",
      "100%|| 835/835 [01:55<00:00,  7.25it/s]\n",
      "100%|| 836/836 [01:29<00:00,  9.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ \n",
    "I started with filters (16, 32, 64, 128), relu activation, padding=same. The test accuracy was really low (around 4%). So I added second layers for each filters type and started with 32 filters. However, total parameters were really huge so I added max pooling 2D and batch normalization to increase speed. To increase performace and avoid local minimums, I added dropout layers. Finally I Flattened the array and used fully connected layer. Softmax activation was used with 133 nodes in the final output as there are 133 types of unique dog breeds in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_245 (Conv2D)          (None, 224, 224, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_234 (MaxPoolin (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_246 (Conv2D)          (None, 112, 112, 32)      4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_235 (MaxPoolin (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 56, 56, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_247 (Conv2D)          (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_236 (MaxPoolin (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_248 (Conv2D)          (None, 28, 28, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_237 (MaxPoolin (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_249 (Conv2D)          (None, 14, 14, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_238 (MaxPoolin (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_250 (Conv2D)          (None, 7, 7, 128)         65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_239 (MaxPoolin (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 786,501\n",
      "Trainable params: 786,437\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 2, activation='relu', padding = 'same', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, 2, activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, 2, activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, 2, activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, 2, activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, 2, activation='relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation = 'softmax'))\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - ETA: 5:18 - loss: 3.5144 - acc: 0.180 - ETA: 4:37 - loss: 3.7495 - acc: 0.110 - ETA: 4:26 - loss: 3.7041 - acc: 0.113 - ETA: 4:19 - loss: 3.7272 - acc: 0.110 - ETA: 4:12 - loss: 3.7118 - acc: 0.112 - ETA: 4:06 - loss: 3.6836 - acc: 0.106 - ETA: 4:04 - loss: 3.6305 - acc: 0.114 - ETA: 4:01 - loss: 3.6414 - acc: 0.115 - ETA: 4:00 - loss: 3.6509 - acc: 0.122 - ETA: 3:56 - loss: 3.6520 - acc: 0.128 - ETA: 3:53 - loss: 3.7188 - acc: 0.123 - ETA: 3:50 - loss: 3.7284 - acc: 0.125 - ETA: 3:47 - loss: 3.7316 - acc: 0.126 - ETA: 3:38 - loss: 3.7187 - acc: 0.126 - ETA: 3:36 - loss: 3.7103 - acc: 0.131 - ETA: 3:34 - loss: 3.7183 - acc: 0.129 - ETA: 3:32 - loss: 3.7330 - acc: 0.125 - ETA: 3:30 - loss: 3.7362 - acc: 0.127 - ETA: 3:28 - loss: 3.7561 - acc: 0.123 - ETA: 3:26 - loss: 3.7505 - acc: 0.124 - ETA: 3:24 - loss: 3.7472 - acc: 0.122 - ETA: 3:23 - loss: 3.7532 - acc: 0.122 - ETA: 3:20 - loss: 3.7642 - acc: 0.119 - ETA: 3:18 - loss: 3.7653 - acc: 0.120 - ETA: 3:16 - loss: 3.7598 - acc: 0.122 - ETA: 3:15 - loss: 3.7610 - acc: 0.120 - ETA: 3:13 - loss: 3.7604 - acc: 0.119 - ETA: 3:11 - loss: 3.7576 - acc: 0.116 - ETA: 3:09 - loss: 3.7614 - acc: 0.118 - ETA: 3:07 - loss: 3.7619 - acc: 0.118 - ETA: 3:05 - loss: 3.7662 - acc: 0.117 - ETA: 3:03 - loss: 3.7663 - acc: 0.117 - ETA: 3:01 - loss: 3.7726 - acc: 0.116 - ETA: 2:59 - loss: 3.7735 - acc: 0.121 - ETA: 2:57 - loss: 3.7657 - acc: 0.122 - ETA: 2:55 - loss: 3.7584 - acc: 0.122 - ETA: 2:54 - loss: 3.7630 - acc: 0.119 - ETA: 2:52 - loss: 3.7546 - acc: 0.123 - ETA: 2:50 - loss: 3.7569 - acc: 0.121 - ETA: 2:48 - loss: 3.7563 - acc: 0.121 - ETA: 2:46 - loss: 3.7551 - acc: 0.119 - ETA: 2:44 - loss: 3.7608 - acc: 0.119 - ETA: 2:42 - loss: 3.7733 - acc: 0.119 - ETA: 2:41 - loss: 3.7684 - acc: 0.120 - ETA: 2:39 - loss: 3.7615 - acc: 0.120 - ETA: 2:37 - loss: 3.7588 - acc: 0.121 - ETA: 2:35 - loss: 3.7656 - acc: 0.120 - ETA: 2:33 - loss: 3.7659 - acc: 0.120 - ETA: 2:31 - loss: 3.7639 - acc: 0.120 - ETA: 2:29 - loss: 3.7669 - acc: 0.119 - ETA: 2:27 - loss: 3.7669 - acc: 0.119 - ETA: 2:26 - loss: 3.7676 - acc: 0.119 - ETA: 2:24 - loss: 3.7710 - acc: 0.119 - ETA: 2:22 - loss: 3.7742 - acc: 0.118 - ETA: 2:20 - loss: 3.7696 - acc: 0.118 - ETA: 2:18 - loss: 3.7656 - acc: 0.119 - ETA: 2:16 - loss: 3.7602 - acc: 0.119 - ETA: 2:15 - loss: 3.7596 - acc: 0.120 - ETA: 2:13 - loss: 3.7594 - acc: 0.119 - ETA: 2:11 - loss: 3.7603 - acc: 0.118 - ETA: 2:09 - loss: 3.7577 - acc: 0.118 - ETA: 2:07 - loss: 3.7567 - acc: 0.117 - ETA: 2:05 - loss: 3.7534 - acc: 0.117 - ETA: 2:04 - loss: 3.7490 - acc: 0.118 - ETA: 2:02 - loss: 3.7515 - acc: 0.117 - ETA: 2:00 - loss: 3.7496 - acc: 0.116 - ETA: 1:58 - loss: 3.7516 - acc: 0.115 - ETA: 1:56 - loss: 3.7535 - acc: 0.115 - ETA: 1:54 - loss: 3.7543 - acc: 0.114 - ETA: 1:53 - loss: 3.7591 - acc: 0.113 - ETA: 1:51 - loss: 3.7615 - acc: 0.112 - ETA: 1:49 - loss: 3.7588 - acc: 0.112 - ETA: 1:47 - loss: 3.7593 - acc: 0.112 - ETA: 1:45 - loss: 3.7608 - acc: 0.112 - ETA: 1:44 - loss: 3.7572 - acc: 0.112 - ETA: 1:42 - loss: 3.7576 - acc: 0.112 - ETA: 1:40 - loss: 3.7575 - acc: 0.112 - ETA: 1:38 - loss: 3.7572 - acc: 0.111 - ETA: 1:36 - loss: 3.7540 - acc: 0.112 - ETA: 1:35 - loss: 3.7532 - acc: 0.113 - ETA: 1:33 - loss: 3.7543 - acc: 0.113 - ETA: 1:31 - loss: 3.7557 - acc: 0.112 - ETA: 1:29 - loss: 3.7535 - acc: 0.112 - ETA: 1:27 - loss: 3.7532 - acc: 0.111 - ETA: 1:26 - loss: 3.7538 - acc: 0.111 - ETA: 1:24 - loss: 3.7491 - acc: 0.113 - ETA: 1:22 - loss: 3.7471 - acc: 0.114 - ETA: 1:20 - loss: 3.7463 - acc: 0.113 - ETA: 1:18 - loss: 3.7453 - acc: 0.114 - ETA: 1:17 - loss: 3.7442 - acc: 0.115 - ETA: 1:15 - loss: 3.7427 - acc: 0.115 - ETA: 1:13 - loss: 3.7458 - acc: 0.115 - ETA: 1:11 - loss: 3.7460 - acc: 0.114 - ETA: 1:09 - loss: 3.7470 - acc: 0.114 - ETA: 1:08 - loss: 3.7473 - acc: 0.114 - ETA: 1:06 - loss: 3.7445 - acc: 0.115 - ETA: 1:04 - loss: 3.7453 - acc: 0.115 - ETA: 1:02 - loss: 3.7463 - acc: 0.115 - ETA: 1:00 - loss: 3.7501 - acc: 0.114 - ETA: 59s - loss: 3.7506 - acc: 0.114 - ETA: 57s - loss: 3.7533 - acc: 0.11 - ETA: 55s - loss: 3.7517 - acc: 0.11 - ETA: 53s - loss: 3.7513 - acc: 0.11 - ETA: 51s - loss: 3.7529 - acc: 0.11 - ETA: 50s - loss: 3.7565 - acc: 0.11 - ETA: 48s - loss: 3.7596 - acc: 0.11 - ETA: 46s - loss: 3.7575 - acc: 0.11 - ETA: 44s - loss: 3.7591 - acc: 0.11 - ETA: 42s - loss: 3.7565 - acc: 0.11 - ETA: 41s - loss: 3.7605 - acc: 0.11 - ETA: 39s - loss: 3.7604 - acc: 0.11 - ETA: 37s - loss: 3.7605 - acc: 0.11 - ETA: 35s - loss: 3.7595 - acc: 0.11 - ETA: 33s - loss: 3.7602 - acc: 0.11 - ETA: 32s - loss: 3.7602 - acc: 0.11 - ETA: 30s - loss: 3.7612 - acc: 0.11 - ETA: 28s - loss: 3.7617 - acc: 0.11 - ETA: 26s - loss: 3.7621 - acc: 0.11 - ETA: 25s - loss: 3.7645 - acc: 0.11 - ETA: 23s - loss: 3.7650 - acc: 0.11 - ETA: 21s - loss: 3.7685 - acc: 0.11 - ETA: 19s - loss: 3.7671 - acc: 0.11 - ETA: 17s - loss: 3.7657 - acc: 0.11 - ETA: 16s - loss: 3.7615 - acc: 0.11 - ETA: 14s - loss: 3.7616 - acc: 0.11 - ETA: 12s - loss: 3.7623 - acc: 0.11 - ETA: 10s - loss: 3.7604 - acc: 0.11 - ETA: 8s - loss: 3.7577 - acc: 0.1129 - ETA: 7s - loss: 3.7600 - acc: 0.113 - ETA: 5s - loss: 3.7604 - acc: 0.112 - ETA: 3s - loss: 3.7575 - acc: 0.112 - ETA: 1s - loss: 3.7551 - acc: 0.113 - 246s 2s/step - loss: 3.7568 - acc: 0.1127 - val_loss: 3.9109 - val_acc: 0.0994\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.91095, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - ETA: 3:50 - loss: 3.6707 - acc: 0.160 - ETA: 3:51 - loss: 3.5996 - acc: 0.160 - ETA: 3:50 - loss: 3.7038 - acc: 0.133 - ETA: 3:49 - loss: 3.6306 - acc: 0.145 - ETA: 3:46 - loss: 3.6190 - acc: 0.128 - ETA: 3:45 - loss: 3.6469 - acc: 0.120 - ETA: 3:44 - loss: 3.6600 - acc: 0.128 - ETA: 3:42 - loss: 3.6279 - acc: 0.127 - ETA: 3:41 - loss: 3.6186 - acc: 0.126 - ETA: 3:39 - loss: 3.6418 - acc: 0.130 - ETA: 3:37 - loss: 3.6281 - acc: 0.127 - ETA: 3:36 - loss: 3.6396 - acc: 0.123 - ETA: 3:34 - loss: 3.6203 - acc: 0.132 - ETA: 3:32 - loss: 3.6123 - acc: 0.135 - ETA: 3:30 - loss: 3.6128 - acc: 0.134 - ETA: 3:29 - loss: 3.6268 - acc: 0.132 - ETA: 3:27 - loss: 3.6531 - acc: 0.128 - ETA: 3:25 - loss: 3.6607 - acc: 0.126 - ETA: 3:23 - loss: 3.6613 - acc: 0.126 - ETA: 3:22 - loss: 3.6547 - acc: 0.124 - ETA: 3:20 - loss: 3.6599 - acc: 0.124 - ETA: 3:18 - loss: 3.6647 - acc: 0.126 - ETA: 3:17 - loss: 3.6578 - acc: 0.127 - ETA: 3:15 - loss: 3.6470 - acc: 0.131 - ETA: 3:13 - loss: 3.6626 - acc: 0.129 - ETA: 3:11 - loss: 3.6661 - acc: 0.127 - ETA: 3:09 - loss: 3.6676 - acc: 0.124 - ETA: 3:08 - loss: 3.6746 - acc: 0.121 - ETA: 3:06 - loss: 3.6698 - acc: 0.121 - ETA: 3:04 - loss: 3.6749 - acc: 0.120 - ETA: 3:02 - loss: 3.6798 - acc: 0.121 - ETA: 3:00 - loss: 3.6812 - acc: 0.121 - ETA: 2:59 - loss: 3.6851 - acc: 0.121 - ETA: 2:57 - loss: 3.6845 - acc: 0.121 - ETA: 2:55 - loss: 3.6856 - acc: 0.121 - ETA: 2:53 - loss: 3.6831 - acc: 0.121 - ETA: 2:51 - loss: 3.6787 - acc: 0.122 - ETA: 2:49 - loss: 3.6820 - acc: 0.121 - ETA: 2:47 - loss: 3.6765 - acc: 0.120 - ETA: 2:46 - loss: 3.6728 - acc: 0.122 - ETA: 2:44 - loss: 3.6710 - acc: 0.123 - ETA: 2:42 - loss: 3.6758 - acc: 0.121 - ETA: 2:40 - loss: 3.6744 - acc: 0.122 - ETA: 2:39 - loss: 3.6744 - acc: 0.122 - ETA: 2:37 - loss: 3.6724 - acc: 0.122 - ETA: 2:35 - loss: 3.6737 - acc: 0.121 - ETA: 2:34 - loss: 3.6776 - acc: 0.120 - ETA: 2:32 - loss: 3.6821 - acc: 0.120 - ETA: 2:30 - loss: 3.6802 - acc: 0.121 - ETA: 2:28 - loss: 3.6831 - acc: 0.120 - ETA: 2:26 - loss: 3.6797 - acc: 0.120 - ETA: 2:25 - loss: 3.6769 - acc: 0.120 - ETA: 2:23 - loss: 3.6750 - acc: 0.121 - ETA: 2:21 - loss: 3.6726 - acc: 0.120 - ETA: 2:18 - loss: 3.6769 - acc: 0.119 - ETA: 2:17 - loss: 3.6787 - acc: 0.118 - ETA: 2:15 - loss: 3.6814 - acc: 0.117 - ETA: 2:13 - loss: 3.6846 - acc: 0.117 - ETA: 2:11 - loss: 3.6932 - acc: 0.116 - ETA: 2:09 - loss: 3.6918 - acc: 0.117 - ETA: 2:08 - loss: 3.6890 - acc: 0.117 - ETA: 2:06 - loss: 3.6873 - acc: 0.118 - ETA: 2:04 - loss: 3.6892 - acc: 0.116 - ETA: 2:02 - loss: 3.6938 - acc: 0.116 - ETA: 2:01 - loss: 3.6919 - acc: 0.118 - ETA: 1:59 - loss: 3.6932 - acc: 0.117 - ETA: 1:57 - loss: 3.6952 - acc: 0.116 - ETA: 1:55 - loss: 3.6916 - acc: 0.117 - ETA: 1:53 - loss: 3.6907 - acc: 0.118 - ETA: 1:52 - loss: 3.6920 - acc: 0.118 - ETA: 1:50 - loss: 3.6918 - acc: 0.118 - ETA: 1:48 - loss: 3.6948 - acc: 0.118 - ETA: 1:46 - loss: 3.6939 - acc: 0.118 - ETA: 1:44 - loss: 3.6903 - acc: 0.118 - ETA: 1:43 - loss: 3.6942 - acc: 0.118 - ETA: 1:41 - loss: 3.6957 - acc: 0.117 - ETA: 1:39 - loss: 3.6998 - acc: 0.118 - ETA: 1:37 - loss: 3.7017 - acc: 0.117 - ETA: 1:36 - loss: 3.6997 - acc: 0.117 - ETA: 1:34 - loss: 3.7004 - acc: 0.117 - ETA: 1:32 - loss: 3.6982 - acc: 0.117 - ETA: 1:30 - loss: 3.6916 - acc: 0.118 - ETA: 1:28 - loss: 3.6906 - acc: 0.118 - ETA: 1:27 - loss: 3.6882 - acc: 0.117 - ETA: 1:25 - loss: 3.6918 - acc: 0.117 - ETA: 1:23 - loss: 3.6910 - acc: 0.117 - ETA: 1:21 - loss: 3.6923 - acc: 0.116 - ETA: 1:20 - loss: 3.6887 - acc: 0.117 - ETA: 1:18 - loss: 3.6868 - acc: 0.117 - ETA: 1:16 - loss: 3.6858 - acc: 0.117 - ETA: 1:14 - loss: 3.6860 - acc: 0.117 - ETA: 1:12 - loss: 3.6906 - acc: 0.117 - ETA: 1:11 - loss: 3.6912 - acc: 0.117 - ETA: 1:09 - loss: 3.6920 - acc: 0.117 - ETA: 1:07 - loss: 3.6936 - acc: 0.117 - ETA: 1:05 - loss: 3.6953 - acc: 0.117 - ETA: 1:04 - loss: 3.6947 - acc: 0.117 - ETA: 1:02 - loss: 3.6931 - acc: 0.117 - ETA: 1:00 - loss: 3.6933 - acc: 0.117 - ETA: 58s - loss: 3.6942 - acc: 0.117 - ETA: 56s - loss: 3.6965 - acc: 0.11 - ETA: 55s - loss: 3.6931 - acc: 0.11 - ETA: 53s - loss: 3.6926 - acc: 0.11 - ETA: 51s - loss: 3.6905 - acc: 0.11 - ETA: 49s - loss: 3.6881 - acc: 0.11 - ETA: 48s - loss: 3.6912 - acc: 0.11 - ETA: 46s - loss: 3.6929 - acc: 0.11 - ETA: 44s - loss: 3.6959 - acc: 0.11 - ETA: 42s - loss: 3.6959 - acc: 0.11 - ETA: 40s - loss: 3.6940 - acc: 0.11 - ETA: 39s - loss: 3.6947 - acc: 0.11 - ETA: 37s - loss: 3.6966 - acc: 0.11 - ETA: 35s - loss: 3.6972 - acc: 0.11 - ETA: 33s - loss: 3.6970 - acc: 0.11 - ETA: 32s - loss: 3.6959 - acc: 0.11 - ETA: 30s - loss: 3.6982 - acc: 0.11 - ETA: 28s - loss: 3.6987 - acc: 0.11 - ETA: 26s - loss: 3.6978 - acc: 0.11 - ETA: 24s - loss: 3.6951 - acc: 0.11 - ETA: 23s - loss: 3.6928 - acc: 0.11 - ETA: 21s - loss: 3.6942 - acc: 0.11 - ETA: 19s - loss: 3.6928 - acc: 0.11 - ETA: 17s - loss: 3.6954 - acc: 0.11 - ETA: 16s - loss: 3.6982 - acc: 0.11 - ETA: 14s - loss: 3.6978 - acc: 0.11 - ETA: 12s - loss: 3.6994 - acc: 0.11 - ETA: 10s - loss: 3.6986 - acc: 0.11 - ETA: 8s - loss: 3.6955 - acc: 0.1171 - ETA: 7s - loss: 3.6961 - acc: 0.116 - ETA: 5s - loss: 3.6942 - acc: 0.117 - ETA: 3s - loss: 3.6915 - acc: 0.117 - ETA: 1s - loss: 3.6964 - acc: 0.117 - 245s 2s/step - loss: 3.6981 - acc: 0.1176 - val_loss: 3.7135 - val_acc: 0.1234\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.91095 to 3.71346, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 3:52 - loss: 3.6280 - acc: 0.140 - ETA: 3:52 - loss: 3.6497 - acc: 0.130 - ETA: 3:51 - loss: 3.6027 - acc: 0.140 - ETA: 3:49 - loss: 3.5821 - acc: 0.155 - ETA: 3:49 - loss: 3.6565 - acc: 0.136 - ETA: 3:47 - loss: 3.6717 - acc: 0.126 - ETA: 3:45 - loss: 3.6727 - acc: 0.131 - ETA: 3:43 - loss: 3.6172 - acc: 0.145 - ETA: 3:41 - loss: 3.6672 - acc: 0.148 - ETA: 3:39 - loss: 3.6395 - acc: 0.156 - ETA: 3:37 - loss: 3.6196 - acc: 0.149 - ETA: 3:35 - loss: 3.6008 - acc: 0.155 - ETA: 3:33 - loss: 3.5898 - acc: 0.152 - ETA: 3:31 - loss: 3.5634 - acc: 0.154 - ETA: 3:30 - loss: 3.5513 - acc: 0.160 - ETA: 3:28 - loss: 3.5636 - acc: 0.158 - ETA: 3:26 - loss: 3.5399 - acc: 0.158 - ETA: 3:24 - loss: 3.5431 - acc: 0.155 - ETA: 3:23 - loss: 3.5380 - acc: 0.153 - ETA: 3:21 - loss: 3.5420 - acc: 0.153 - ETA: 3:19 - loss: 3.5373 - acc: 0.154 - ETA: 3:17 - loss: 3.5356 - acc: 0.154 - ETA: 3:16 - loss: 3.5245 - acc: 0.154 - ETA: 3:14 - loss: 3.5224 - acc: 0.158 - ETA: 3:12 - loss: 3.5226 - acc: 0.156 - ETA: 3:11 - loss: 3.5240 - acc: 0.153 - ETA: 3:09 - loss: 3.5319 - acc: 0.152 - ETA: 3:07 - loss: 3.5320 - acc: 0.152 - ETA: 3:05 - loss: 3.5488 - acc: 0.150 - ETA: 3:03 - loss: 3.5494 - acc: 0.148 - ETA: 3:02 - loss: 3.5567 - acc: 0.147 - ETA: 2:58 - loss: 3.5628 - acc: 0.145 - ETA: 2:56 - loss: 3.5697 - acc: 0.144 - ETA: 2:54 - loss: 3.5715 - acc: 0.144 - ETA: 2:52 - loss: 3.5710 - acc: 0.142 - ETA: 2:51 - loss: 3.5665 - acc: 0.144 - ETA: 2:49 - loss: 3.5661 - acc: 0.145 - ETA: 2:47 - loss: 3.5769 - acc: 0.144 - ETA: 2:45 - loss: 3.5651 - acc: 0.146 - ETA: 2:44 - loss: 3.5682 - acc: 0.144 - ETA: 2:42 - loss: 3.5707 - acc: 0.143 - ETA: 2:40 - loss: 3.5735 - acc: 0.143 - ETA: 2:38 - loss: 3.5697 - acc: 0.145 - ETA: 2:37 - loss: 3.5677 - acc: 0.145 - ETA: 2:35 - loss: 3.5690 - acc: 0.144 - ETA: 2:33 - loss: 3.5728 - acc: 0.143 - ETA: 2:31 - loss: 3.5733 - acc: 0.143 - ETA: 2:30 - loss: 3.5769 - acc: 0.142 - ETA: 2:28 - loss: 3.5850 - acc: 0.142 - ETA: 2:26 - loss: 3.5907 - acc: 0.142 - ETA: 2:24 - loss: 3.5932 - acc: 0.142 - ETA: 2:23 - loss: 3.5941 - acc: 0.141 - ETA: 2:21 - loss: 3.5970 - acc: 0.140 - ETA: 2:19 - loss: 3.6038 - acc: 0.139 - ETA: 2:17 - loss: 3.6062 - acc: 0.138 - ETA: 2:16 - loss: 3.6111 - acc: 0.136 - ETA: 2:14 - loss: 3.6137 - acc: 0.137 - ETA: 2:12 - loss: 3.6129 - acc: 0.136 - ETA: 2:10 - loss: 3.6194 - acc: 0.135 - ETA: 2:09 - loss: 3.6200 - acc: 0.136 - ETA: 2:07 - loss: 3.6224 - acc: 0.135 - ETA: 2:05 - loss: 3.6181 - acc: 0.135 - ETA: 2:03 - loss: 3.6179 - acc: 0.134 - ETA: 2:02 - loss: 3.6173 - acc: 0.134 - ETA: 2:00 - loss: 3.6180 - acc: 0.134 - ETA: 1:58 - loss: 3.6184 - acc: 0.133 - ETA: 1:56 - loss: 3.6208 - acc: 0.132 - ETA: 1:55 - loss: 3.6264 - acc: 0.131 - ETA: 1:53 - loss: 3.6256 - acc: 0.131 - ETA: 1:51 - loss: 3.6280 - acc: 0.130 - ETA: 1:49 - loss: 3.6306 - acc: 0.130 - ETA: 1:48 - loss: 3.6308 - acc: 0.130 - ETA: 1:46 - loss: 3.6305 - acc: 0.129 - ETA: 1:44 - loss: 3.6291 - acc: 0.129 - ETA: 1:42 - loss: 3.6327 - acc: 0.129 - ETA: 1:40 - loss: 3.6350 - acc: 0.129 - ETA: 1:39 - loss: 3.6375 - acc: 0.130 - ETA: 1:37 - loss: 3.6378 - acc: 0.131 - ETA: 1:37 - loss: 3.6347 - acc: 0.131 - ETA: 1:35 - loss: 3.6348 - acc: 0.131 - ETA: 1:33 - loss: 3.6363 - acc: 0.131 - ETA: 1:31 - loss: 3.6344 - acc: 0.132 - ETA: 1:29 - loss: 3.6375 - acc: 0.131 - ETA: 1:28 - loss: 3.6393 - acc: 0.130 - ETA: 1:26 - loss: 3.6392 - acc: 0.130 - ETA: 1:24 - loss: 3.6400 - acc: 0.131 - ETA: 1:22 - loss: 3.6388 - acc: 0.131 - ETA: 1:20 - loss: 3.6366 - acc: 0.131 - ETA: 1:19 - loss: 3.6385 - acc: 0.131 - ETA: 1:17 - loss: 3.6409 - acc: 0.130 - ETA: 1:15 - loss: 3.6442 - acc: 0.130 - ETA: 1:13 - loss: 3.6463 - acc: 0.129 - ETA: 1:11 - loss: 3.6472 - acc: 0.129 - ETA: 1:10 - loss: 3.6468 - acc: 0.128 - ETA: 1:08 - loss: 3.6461 - acc: 0.128 - ETA: 1:06 - loss: 3.6434 - acc: 0.129 - ETA: 1:04 - loss: 3.6398 - acc: 0.130 - ETA: 1:02 - loss: 3.6382 - acc: 0.130 - ETA: 1:01 - loss: 3.6413 - acc: 0.130 - ETA: 59s - loss: 3.6441 - acc: 0.130 - ETA: 57s - loss: 3.6421 - acc: 0.13 - ETA: 55s - loss: 3.6415 - acc: 0.13 - ETA: 53s - loss: 3.6465 - acc: 0.12 - ETA: 52s - loss: 3.6482 - acc: 0.12 - ETA: 50s - loss: 3.6446 - acc: 0.13 - ETA: 48s - loss: 3.6445 - acc: 0.13 - ETA: 46s - loss: 3.6415 - acc: 0.13 - ETA: 44s - loss: 3.6441 - acc: 0.13 - ETA: 43s - loss: 3.6424 - acc: 0.13 - ETA: 41s - loss: 3.6407 - acc: 0.13 - ETA: 39s - loss: 3.6410 - acc: 0.13 - ETA: 37s - loss: 3.6403 - acc: 0.13 - ETA: 35s - loss: 3.6411 - acc: 0.13 - ETA: 34s - loss: 3.6394 - acc: 0.13 - ETA: 32s - loss: 3.6416 - acc: 0.13 - ETA: 30s - loss: 3.6418 - acc: 0.13 - ETA: 28s - loss: 3.6441 - acc: 0.12 - ETA: 26s - loss: 3.6441 - acc: 0.12 - ETA: 25s - loss: 3.6450 - acc: 0.12 - ETA: 23s - loss: 3.6470 - acc: 0.12 - ETA: 21s - loss: 3.6426 - acc: 0.12 - ETA: 19s - loss: 3.6440 - acc: 0.12 - ETA: 17s - loss: 3.6461 - acc: 0.12 - ETA: 16s - loss: 3.6419 - acc: 0.12 - ETA: 14s - loss: 3.6443 - acc: 0.12 - ETA: 12s - loss: 3.6450 - acc: 0.12 - ETA: 10s - loss: 3.6455 - acc: 0.12 - ETA: 8s - loss: 3.6444 - acc: 0.1275 - ETA: 7s - loss: 3.6422 - acc: 0.127 - ETA: 5s - loss: 3.6435 - acc: 0.128 - ETA: 3s - loss: 3.6458 - acc: 0.128 - ETA: 1s - loss: 3.6449 - acc: 0.128 - 248s 2s/step - loss: 3.6447 - acc: 0.1289 - val_loss: 3.9142 - val_acc: 0.0982\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - ETA: 3:42 - loss: 3.9810 - acc: 0.060 - ETA: 3:41 - loss: 3.7852 - acc: 0.100 - ETA: 3:43 - loss: 3.7519 - acc: 0.126 - ETA: 3:44 - loss: 3.7351 - acc: 0.130 - ETA: 3:43 - loss: 3.6879 - acc: 0.132 - ETA: 3:43 - loss: 3.6329 - acc: 0.130 - ETA: 3:42 - loss: 3.5627 - acc: 0.140 - ETA: 3:41 - loss: 3.5871 - acc: 0.130 - ETA: 3:40 - loss: 3.5932 - acc: 0.133 - ETA: 3:38 - loss: 3.6449 - acc: 0.130 - ETA: 3:37 - loss: 3.6241 - acc: 0.123 - ETA: 3:35 - loss: 3.6055 - acc: 0.118 - ETA: 3:33 - loss: 3.5933 - acc: 0.116 - ETA: 3:32 - loss: 3.5624 - acc: 0.124 - ETA: 3:31 - loss: 3.5414 - acc: 0.126 - ETA: 3:29 - loss: 3.5364 - acc: 0.125 - ETA: 3:28 - loss: 3.5494 - acc: 0.124 - ETA: 3:26 - loss: 3.5535 - acc: 0.130 - ETA: 3:24 - loss: 3.5591 - acc: 0.126 - ETA: 3:23 - loss: 3.5707 - acc: 0.122 - ETA: 3:21 - loss: 3.5787 - acc: 0.120 - ETA: 3:20 - loss: 3.5824 - acc: 0.123 - ETA: 3:18 - loss: 3.5851 - acc: 0.121 - ETA: 3:16 - loss: 3.5904 - acc: 0.120 - ETA: 3:14 - loss: 3.5963 - acc: 0.121 - ETA: 3:12 - loss: 3.5933 - acc: 0.123 - ETA: 3:11 - loss: 3.5814 - acc: 0.125 - ETA: 3:09 - loss: 3.5751 - acc: 0.125 - ETA: 3:07 - loss: 3.5728 - acc: 0.126 - ETA: 3:05 - loss: 3.5703 - acc: 0.127 - ETA: 3:03 - loss: 3.5766 - acc: 0.125 - ETA: 3:02 - loss: 3.5778 - acc: 0.125 - ETA: 3:00 - loss: 3.5852 - acc: 0.125 - ETA: 2:58 - loss: 3.5922 - acc: 0.125 - ETA: 2:55 - loss: 3.5927 - acc: 0.127 - ETA: 2:53 - loss: 3.5918 - acc: 0.127 - ETA: 2:51 - loss: 3.5974 - acc: 0.126 - ETA: 2:49 - loss: 3.6047 - acc: 0.124 - ETA: 2:48 - loss: 3.5975 - acc: 0.126 - ETA: 2:46 - loss: 3.5968 - acc: 0.127 - ETA: 2:44 - loss: 3.5945 - acc: 0.126 - ETA: 2:42 - loss: 3.6029 - acc: 0.126 - ETA: 2:41 - loss: 3.6010 - acc: 0.125 - ETA: 2:39 - loss: 3.6032 - acc: 0.126 - ETA: 2:37 - loss: 3.6116 - acc: 0.126 - ETA: 2:35 - loss: 3.6182 - acc: 0.126 - ETA: 2:34 - loss: 3.6205 - acc: 0.126 - ETA: 2:32 - loss: 3.6268 - acc: 0.125 - ETA: 2:30 - loss: 3.6277 - acc: 0.124 - ETA: 2:28 - loss: 3.6229 - acc: 0.124 - ETA: 2:27 - loss: 3.6181 - acc: 0.124 - ETA: 2:25 - loss: 3.6200 - acc: 0.125 - ETA: 2:23 - loss: 3.6221 - acc: 0.124 - ETA: 2:21 - loss: 3.6198 - acc: 0.124 - ETA: 2:19 - loss: 3.6163 - acc: 0.125 - ETA: 2:18 - loss: 3.6134 - acc: 0.126 - ETA: 2:16 - loss: 3.6088 - acc: 0.126 - ETA: 2:14 - loss: 3.6111 - acc: 0.124 - ETA: 2:12 - loss: 3.6146 - acc: 0.125 - ETA: 2:10 - loss: 3.6180 - acc: 0.123 - ETA: 2:09 - loss: 3.6197 - acc: 0.123 - ETA: 2:07 - loss: 3.6232 - acc: 0.123 - ETA: 2:05 - loss: 3.6212 - acc: 0.124 - ETA: 2:03 - loss: 3.6227 - acc: 0.123 - ETA: 2:02 - loss: 3.6236 - acc: 0.124 - ETA: 2:00 - loss: 3.6261 - acc: 0.123 - ETA: 1:58 - loss: 3.6292 - acc: 0.123 - ETA: 1:56 - loss: 3.6323 - acc: 0.122 - ETA: 1:54 - loss: 3.6325 - acc: 0.122 - ETA: 1:53 - loss: 3.6374 - acc: 0.122 - ETA: 1:51 - loss: 3.6361 - acc: 0.123 - ETA: 1:49 - loss: 3.6320 - acc: 0.123 - ETA: 1:47 - loss: 3.6313 - acc: 0.124 - ETA: 1:45 - loss: 3.6305 - acc: 0.124 - ETA: 1:44 - loss: 3.6316 - acc: 0.123 - ETA: 1:42 - loss: 3.6263 - acc: 0.123 - ETA: 1:40 - loss: 3.6241 - acc: 0.123 - ETA: 1:38 - loss: 3.6232 - acc: 0.123 - ETA: 1:36 - loss: 3.6234 - acc: 0.122 - ETA: 1:35 - loss: 3.6229 - acc: 0.121 - ETA: 1:33 - loss: 3.6200 - acc: 0.121 - ETA: 1:31 - loss: 3.6226 - acc: 0.122 - ETA: 1:29 - loss: 3.6265 - acc: 0.121 - ETA: 1:28 - loss: 3.6264 - acc: 0.121 - ETA: 1:26 - loss: 3.6267 - acc: 0.122 - ETA: 1:24 - loss: 3.6261 - acc: 0.122 - ETA: 1:22 - loss: 3.6237 - acc: 0.122 - ETA: 1:20 - loss: 3.6275 - acc: 0.122 - ETA: 1:19 - loss: 3.6287 - acc: 0.122 - ETA: 1:17 - loss: 3.6270 - acc: 0.122 - ETA: 1:15 - loss: 3.6241 - acc: 0.123 - ETA: 1:13 - loss: 3.6223 - acc: 0.123 - ETA: 1:11 - loss: 3.6240 - acc: 0.123 - ETA: 1:10 - loss: 3.6228 - acc: 0.123 - ETA: 1:08 - loss: 3.6258 - acc: 0.123 - ETA: 1:06 - loss: 3.6225 - acc: 0.123 - ETA: 1:04 - loss: 3.6197 - acc: 0.124 - ETA: 1:02 - loss: 3.6207 - acc: 0.123 - ETA: 1:01 - loss: 3.6250 - acc: 0.123 - ETA: 59s - loss: 3.6221 - acc: 0.124 - ETA: 57s - loss: 3.6199 - acc: 0.12 - ETA: 55s - loss: 3.6208 - acc: 0.12 - ETA: 53s - loss: 3.6208 - acc: 0.12 - ETA: 52s - loss: 3.6262 - acc: 0.12 - ETA: 50s - loss: 3.6286 - acc: 0.12 - ETA: 48s - loss: 3.6316 - acc: 0.12 - ETA: 46s - loss: 3.6322 - acc: 0.12 - ETA: 44s - loss: 3.6327 - acc: 0.12 - ETA: 43s - loss: 3.6344 - acc: 0.12 - ETA: 41s - loss: 3.6320 - acc: 0.12 - ETA: 39s - loss: 3.6319 - acc: 0.12 - ETA: 37s - loss: 3.6298 - acc: 0.12 - ETA: 35s - loss: 3.6283 - acc: 0.12 - ETA: 34s - loss: 3.6282 - acc: 0.12 - ETA: 32s - loss: 3.6260 - acc: 0.12 - ETA: 30s - loss: 3.6255 - acc: 0.12 - ETA: 28s - loss: 3.6272 - acc: 0.12 - ETA: 27s - loss: 3.6246 - acc: 0.12 - ETA: 25s - loss: 3.6233 - acc: 0.12 - ETA: 23s - loss: 3.6213 - acc: 0.12 - ETA: 21s - loss: 3.6206 - acc: 0.12 - ETA: 19s - loss: 3.6190 - acc: 0.12 - ETA: 18s - loss: 3.6210 - acc: 0.12 - ETA: 16s - loss: 3.6219 - acc: 0.12 - ETA: 14s - loss: 3.6249 - acc: 0.12 - ETA: 12s - loss: 3.6230 - acc: 0.12 - ETA: 10s - loss: 3.6230 - acc: 0.12 - ETA: 8s - loss: 3.6230 - acc: 0.1266 - ETA: 7s - loss: 3.6245 - acc: 0.126 - ETA: 5s - loss: 3.6271 - acc: 0.126 - ETA: 3s - loss: 3.6254 - acc: 0.126 - ETA: 1s - loss: 3.6260 - acc: 0.126 - 249s 2s/step - loss: 3.6291 - acc: 0.1266 - val_loss: 3.8781 - val_acc: 0.0934\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 3:43 - loss: 3.7028 - acc: 0.080 - ETA: 3:45 - loss: 3.7801 - acc: 0.060 - ETA: 3:43 - loss: 3.7311 - acc: 0.100 - ETA: 3:44 - loss: 3.7180 - acc: 0.125 - ETA: 3:46 - loss: 3.6381 - acc: 0.136 - ETA: 3:44 - loss: 3.6324 - acc: 0.126 - ETA: 3:43 - loss: 3.5816 - acc: 0.134 - ETA: 3:41 - loss: 3.5376 - acc: 0.140 - ETA: 3:40 - loss: 3.5582 - acc: 0.142 - ETA: 3:38 - loss: 3.5367 - acc: 0.146 - ETA: 3:37 - loss: 3.5639 - acc: 0.140 - ETA: 3:36 - loss: 3.5818 - acc: 0.136 - ETA: 3:34 - loss: 3.5869 - acc: 0.141 - ETA: 3:33 - loss: 3.5905 - acc: 0.148 - ETA: 3:32 - loss: 3.5676 - acc: 0.148 - ETA: 3:30 - loss: 3.5725 - acc: 0.148 - ETA: 3:28 - loss: 3.5636 - acc: 0.149 - ETA: 3:26 - loss: 3.5712 - acc: 0.148 - ETA: 3:25 - loss: 3.5568 - acc: 0.150 - ETA: 3:23 - loss: 3.5467 - acc: 0.153 - ETA: 3:21 - loss: 3.5439 - acc: 0.150 - ETA: 3:19 - loss: 3.5548 - acc: 0.149 - ETA: 3:17 - loss: 3.5604 - acc: 0.147 - ETA: 3:15 - loss: 3.5383 - acc: 0.147 - ETA: 3:14 - loss: 3.5532 - acc: 0.145 - ETA: 3:12 - loss: 3.5500 - acc: 0.146 - ETA: 3:10 - loss: 3.5552 - acc: 0.145 - ETA: 3:08 - loss: 3.5501 - acc: 0.147 - ETA: 3:07 - loss: 3.5602 - acc: 0.143 - ETA: 3:05 - loss: 3.5636 - acc: 0.141 - ETA: 3:03 - loss: 3.5572 - acc: 0.143 - ETA: 3:01 - loss: 3.5515 - acc: 0.143 - ETA: 2:59 - loss: 3.5504 - acc: 0.143 - ETA: 2:58 - loss: 3.5617 - acc: 0.140 - ETA: 2:56 - loss: 3.5675 - acc: 0.138 - ETA: 2:54 - loss: 3.5668 - acc: 0.138 - ETA: 2:52 - loss: 3.5742 - acc: 0.136 - ETA: 2:51 - loss: 3.5732 - acc: 0.137 - ETA: 2:49 - loss: 3.5725 - acc: 0.137 - ETA: 2:47 - loss: 3.5722 - acc: 0.138 - ETA: 2:45 - loss: 3.5751 - acc: 0.137 - ETA: 2:43 - loss: 3.5748 - acc: 0.135 - ETA: 2:41 - loss: 3.5656 - acc: 0.138 - ETA: 2:39 - loss: 3.5654 - acc: 0.138 - ETA: 2:38 - loss: 3.5618 - acc: 0.138 - ETA: 2:36 - loss: 3.5631 - acc: 0.139 - ETA: 2:34 - loss: 3.5752 - acc: 0.137 - ETA: 2:32 - loss: 3.5743 - acc: 0.137 - ETA: 2:30 - loss: 3.5772 - acc: 0.137 - ETA: 2:28 - loss: 3.5767 - acc: 0.138 - ETA: 2:27 - loss: 3.5757 - acc: 0.137 - ETA: 2:25 - loss: 3.5732 - acc: 0.138 - ETA: 2:23 - loss: 3.5646 - acc: 0.139 - ETA: 2:21 - loss: 3.5671 - acc: 0.139 - ETA: 2:19 - loss: 3.5659 - acc: 0.138 - ETA: 2:18 - loss: 3.5698 - acc: 0.137 - ETA: 2:16 - loss: 3.5715 - acc: 0.137 - ETA: 2:14 - loss: 3.5689 - acc: 0.137 - ETA: 2:12 - loss: 3.5752 - acc: 0.136 - ETA: 2:11 - loss: 3.5748 - acc: 0.136 - ETA: 2:09 - loss: 3.5757 - acc: 0.135 - ETA: 2:07 - loss: 3.5780 - acc: 0.135 - ETA: 2:05 - loss: 3.5822 - acc: 0.135 - ETA: 2:03 - loss: 3.5823 - acc: 0.136 - ETA: 2:02 - loss: 3.5783 - acc: 0.136 - ETA: 2:00 - loss: 3.5802 - acc: 0.136 - ETA: 1:58 - loss: 3.5816 - acc: 0.136 - ETA: 1:56 - loss: 3.5808 - acc: 0.137 - ETA: 1:54 - loss: 3.5801 - acc: 0.137 - ETA: 1:53 - loss: 3.5790 - acc: 0.137 - ETA: 1:51 - loss: 3.5841 - acc: 0.138 - ETA: 1:49 - loss: 3.5861 - acc: 0.138 - ETA: 1:47 - loss: 3.5872 - acc: 0.139 - ETA: 1:45 - loss: 3.5814 - acc: 0.140 - ETA: 1:44 - loss: 3.5838 - acc: 0.139 - ETA: 1:42 - loss: 3.5832 - acc: 0.139 - ETA: 1:40 - loss: 3.5817 - acc: 0.139 - ETA: 1:38 - loss: 3.5812 - acc: 0.139 - ETA: 1:36 - loss: 3.5784 - acc: 0.139 - ETA: 1:35 - loss: 3.5814 - acc: 0.138 - ETA: 1:33 - loss: 3.5809 - acc: 0.137 - ETA: 1:31 - loss: 3.5810 - acc: 0.137 - ETA: 1:29 - loss: 3.5816 - acc: 0.137 - ETA: 1:27 - loss: 3.5835 - acc: 0.136 - ETA: 1:25 - loss: 3.5816 - acc: 0.137 - ETA: 1:24 - loss: 3.5811 - acc: 0.138 - ETA: 1:22 - loss: 3.5774 - acc: 0.137 - ETA: 1:20 - loss: 3.5764 - acc: 0.138 - ETA: 1:18 - loss: 3.5757 - acc: 0.138 - ETA: 1:17 - loss: 3.5717 - acc: 0.139 - ETA: 1:15 - loss: 3.5672 - acc: 0.139 - ETA: 1:13 - loss: 3.5677 - acc: 0.138 - ETA: 1:11 - loss: 3.5664 - acc: 0.139 - ETA: 1:09 - loss: 3.5690 - acc: 0.138 - ETA: 1:08 - loss: 3.5697 - acc: 0.138 - ETA: 1:06 - loss: 3.5712 - acc: 0.138 - ETA: 1:04 - loss: 3.5731 - acc: 0.138 - ETA: 1:02 - loss: 3.5730 - acc: 0.138 - ETA: 1:00 - loss: 3.5751 - acc: 0.137 - ETA: 59s - loss: 3.5748 - acc: 0.138 - ETA: 57s - loss: 3.5763 - acc: 0.13 - ETA: 55s - loss: 3.5742 - acc: 0.13 - ETA: 53s - loss: 3.5754 - acc: 0.13 - ETA: 51s - loss: 3.5751 - acc: 0.13 - ETA: 50s - loss: 3.5758 - acc: 0.13 - ETA: 48s - loss: 3.5773 - acc: 0.13 - ETA: 46s - loss: 3.5757 - acc: 0.13 - ETA: 44s - loss: 3.5733 - acc: 0.13 - ETA: 42s - loss: 3.5703 - acc: 0.13 - ETA: 41s - loss: 3.5694 - acc: 0.14 - ETA: 39s - loss: 3.5715 - acc: 0.13 - ETA: 37s - loss: 3.5713 - acc: 0.14 - ETA: 35s - loss: 3.5695 - acc: 0.14 - ETA: 33s - loss: 3.5651 - acc: 0.14 - ETA: 32s - loss: 3.5665 - acc: 0.14 - ETA: 30s - loss: 3.5632 - acc: 0.14 - ETA: 28s - loss: 3.5618 - acc: 0.14 - ETA: 26s - loss: 3.5620 - acc: 0.14 - ETA: 25s - loss: 3.5621 - acc: 0.14 - ETA: 23s - loss: 3.5602 - acc: 0.14 - ETA: 21s - loss: 3.5589 - acc: 0.14 - ETA: 19s - loss: 3.5635 - acc: 0.14 - ETA: 17s - loss: 3.5631 - acc: 0.14 - ETA: 16s - loss: 3.5636 - acc: 0.14 - ETA: 14s - loss: 3.5631 - acc: 0.14 - ETA: 12s - loss: 3.5606 - acc: 0.14 - ETA: 10s - loss: 3.5583 - acc: 0.14 - ETA: 8s - loss: 3.5555 - acc: 0.1428 - ETA: 7s - loss: 3.5539 - acc: 0.142 - ETA: 5s - loss: 3.5542 - acc: 0.143 - ETA: 3s - loss: 3.5547 - acc: 0.142 - ETA: 1s - loss: 3.5543 - acc: 0.143 - 248s 2s/step - loss: 3.5545 - acc: 0.1433 - val_loss: 4.3455 - val_acc: 0.0719\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - ETA: 3:42 - loss: 3.7572 - acc: 0.100 - ETA: 2:58 - loss: 3.8501 - acc: 0.133 - ETA: 3:11 - loss: 3.7734 - acc: 0.148 - ETA: 3:18 - loss: 3.7765 - acc: 0.141 - ETA: 3:23 - loss: 3.6925 - acc: 0.141 - ETA: 3:26 - loss: 3.6633 - acc: 0.144 - ETA: 3:28 - loss: 3.6037 - acc: 0.163 - ETA: 3:29 - loss: 3.5787 - acc: 0.160 - ETA: 3:19 - loss: 3.5787 - acc: 0.165 - ETA: 3:20 - loss: 3.5733 - acc: 0.164 - ETA: 3:20 - loss: 3.5587 - acc: 0.167 - ETA: 3:20 - loss: 3.5763 - acc: 0.163 - ETA: 3:20 - loss: 3.6015 - acc: 0.159 - ETA: 3:19 - loss: 3.5802 - acc: 0.161 - ETA: 3:19 - loss: 3.5781 - acc: 0.160 - ETA: 3:18 - loss: 3.5601 - acc: 0.164 - ETA: 3:17 - loss: 3.5447 - acc: 0.166 - ETA: 3:16 - loss: 3.5406 - acc: 0.162 - ETA: 3:14 - loss: 3.5380 - acc: 0.155 - ETA: 3:13 - loss: 3.5273 - acc: 0.157 - ETA: 3:12 - loss: 3.5235 - acc: 0.154 - ETA: 3:11 - loss: 3.5145 - acc: 0.155 - ETA: 3:09 - loss: 3.5142 - acc: 0.155 - ETA: 3:08 - loss: 3.5119 - acc: 0.156 - ETA: 3:07 - loss: 3.5285 - acc: 0.157 - ETA: 3:05 - loss: 3.5298 - acc: 0.157 - ETA: 3:04 - loss: 3.5480 - acc: 0.153 - ETA: 3:02 - loss: 3.5479 - acc: 0.153 - ETA: 3:00 - loss: 3.5475 - acc: 0.151 - ETA: 2:59 - loss: 3.5522 - acc: 0.151 - ETA: 2:57 - loss: 3.5624 - acc: 0.152 - ETA: 2:56 - loss: 3.5500 - acc: 0.154 - ETA: 2:55 - loss: 3.5488 - acc: 0.153 - ETA: 2:53 - loss: 3.5492 - acc: 0.152 - ETA: 2:52 - loss: 3.5431 - acc: 0.151 - ETA: 2:50 - loss: 3.5414 - acc: 0.153 - ETA: 2:48 - loss: 3.5405 - acc: 0.151 - ETA: 2:47 - loss: 3.5436 - acc: 0.150 - ETA: 2:45 - loss: 3.5450 - acc: 0.151 - ETA: 2:43 - loss: 3.5485 - acc: 0.149 - ETA: 2:42 - loss: 3.5495 - acc: 0.148 - ETA: 2:40 - loss: 3.5541 - acc: 0.148 - ETA: 2:38 - loss: 3.5530 - acc: 0.147 - ETA: 2:36 - loss: 3.5540 - acc: 0.147 - ETA: 2:35 - loss: 3.5585 - acc: 0.147 - ETA: 2:33 - loss: 3.5533 - acc: 0.147 - ETA: 2:31 - loss: 3.5488 - acc: 0.147 - ETA: 2:29 - loss: 3.5494 - acc: 0.146 - ETA: 2:28 - loss: 3.5516 - acc: 0.145 - ETA: 2:26 - loss: 3.5544 - acc: 0.144 - ETA: 2:24 - loss: 3.5592 - acc: 0.144 - ETA: 2:23 - loss: 3.5642 - acc: 0.142 - ETA: 2:21 - loss: 3.5650 - acc: 0.142 - ETA: 2:19 - loss: 3.5573 - acc: 0.143 - ETA: 2:17 - loss: 3.5585 - acc: 0.142 - ETA: 2:15 - loss: 3.5563 - acc: 0.142 - ETA: 2:14 - loss: 3.5558 - acc: 0.143 - ETA: 2:12 - loss: 3.5526 - acc: 0.144 - ETA: 2:10 - loss: 3.5457 - acc: 0.147 - ETA: 2:08 - loss: 3.5418 - acc: 0.147 - ETA: 2:07 - loss: 3.5377 - acc: 0.147 - ETA: 2:05 - loss: 3.5368 - acc: 0.148 - ETA: 2:03 - loss: 3.5426 - acc: 0.149 - ETA: 2:01 - loss: 3.5394 - acc: 0.149 - ETA: 2:00 - loss: 3.5413 - acc: 0.149 - ETA: 1:58 - loss: 3.5431 - acc: 0.148 - ETA: 1:56 - loss: 3.5434 - acc: 0.149 - ETA: 1:54 - loss: 3.5490 - acc: 0.149 - ETA: 1:53 - loss: 3.5494 - acc: 0.149 - ETA: 1:51 - loss: 3.5448 - acc: 0.151 - ETA: 1:49 - loss: 3.5371 - acc: 0.151 - ETA: 1:47 - loss: 3.5388 - acc: 0.152 - ETA: 1:46 - loss: 3.5445 - acc: 0.150 - ETA: 1:44 - loss: 3.5390 - acc: 0.152 - ETA: 1:42 - loss: 3.5375 - acc: 0.152 - ETA: 1:40 - loss: 3.5408 - acc: 0.151 - ETA: 1:39 - loss: 3.5391 - acc: 0.152 - ETA: 1:37 - loss: 3.5396 - acc: 0.152 - ETA: 1:35 - loss: 3.5418 - acc: 0.152 - ETA: 1:33 - loss: 3.5459 - acc: 0.151 - ETA: 1:32 - loss: 3.5459 - acc: 0.151 - ETA: 1:30 - loss: 3.5460 - acc: 0.151 - ETA: 1:28 - loss: 3.5480 - acc: 0.150 - ETA: 1:26 - loss: 3.5504 - acc: 0.149 - ETA: 1:25 - loss: 3.5530 - acc: 0.148 - ETA: 1:23 - loss: 3.5520 - acc: 0.148 - ETA: 1:21 - loss: 3.5509 - acc: 0.147 - ETA: 1:19 - loss: 3.5532 - acc: 0.146 - ETA: 1:18 - loss: 3.5510 - acc: 0.147 - ETA: 1:16 - loss: 3.5513 - acc: 0.146 - ETA: 1:14 - loss: 3.5528 - acc: 0.146 - ETA: 1:12 - loss: 3.5579 - acc: 0.145 - ETA: 1:10 - loss: 3.5602 - acc: 0.144 - ETA: 1:09 - loss: 3.5617 - acc: 0.144 - ETA: 1:07 - loss: 3.5576 - acc: 0.144 - ETA: 1:05 - loss: 3.5567 - acc: 0.143 - ETA: 1:03 - loss: 3.5589 - acc: 0.143 - ETA: 1:02 - loss: 3.5554 - acc: 0.143 - ETA: 1:00 - loss: 3.5513 - acc: 0.144 - ETA: 58s - loss: 3.5490 - acc: 0.144 - ETA: 56s - loss: 3.5488 - acc: 0.14 - ETA: 55s - loss: 3.5500 - acc: 0.14 - ETA: 53s - loss: 3.5495 - acc: 0.14 - ETA: 51s - loss: 3.5484 - acc: 0.14 - ETA: 49s - loss: 3.5488 - acc: 0.14 - ETA: 47s - loss: 3.5494 - acc: 0.14 - ETA: 46s - loss: 3.5499 - acc: 0.14 - ETA: 44s - loss: 3.5470 - acc: 0.14 - ETA: 42s - loss: 3.5505 - acc: 0.14 - ETA: 40s - loss: 3.5530 - acc: 0.14 - ETA: 39s - loss: 3.5538 - acc: 0.14 - ETA: 37s - loss: 3.5518 - acc: 0.14 - ETA: 35s - loss: 3.5506 - acc: 0.14 - ETA: 33s - loss: 3.5478 - acc: 0.14 - ETA: 31s - loss: 3.5467 - acc: 0.14 - ETA: 30s - loss: 3.5422 - acc: 0.14 - ETA: 28s - loss: 3.5410 - acc: 0.14 - ETA: 26s - loss: 3.5393 - acc: 0.14 - ETA: 24s - loss: 3.5445 - acc: 0.14 - ETA: 23s - loss: 3.5410 - acc: 0.14 - ETA: 21s - loss: 3.5408 - acc: 0.14 - ETA: 19s - loss: 3.5446 - acc: 0.14 - ETA: 17s - loss: 3.5448 - acc: 0.14 - ETA: 15s - loss: 3.5443 - acc: 0.14 - ETA: 14s - loss: 3.5447 - acc: 0.14 - ETA: 12s - loss: 3.5438 - acc: 0.14 - ETA: 10s - loss: 3.5416 - acc: 0.14 - ETA: 8s - loss: 3.5411 - acc: 0.1451 - ETA: 7s - loss: 3.5410 - acc: 0.146 - ETA: 5s - loss: 3.5419 - acc: 0.146 - ETA: 3s - loss: 3.5423 - acc: 0.146 - ETA: 1s - loss: 3.5437 - acc: 0.146 - 246s 2s/step - loss: 3.5449 - acc: 0.1462 - val_loss: 3.6559 - val_acc: 0.1293\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.71346 to 3.65587, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 3:46 - loss: 3.3139 - acc: 0.160 - ETA: 3:39 - loss: 3.4250 - acc: 0.190 - ETA: 3:39 - loss: 3.5267 - acc: 0.180 - ETA: 3:38 - loss: 3.5377 - acc: 0.175 - ETA: 3:38 - loss: 3.5175 - acc: 0.168 - ETA: 3:38 - loss: 3.5747 - acc: 0.150 - ETA: 3:38 - loss: 3.5689 - acc: 0.148 - ETA: 3:37 - loss: 3.4884 - acc: 0.165 - ETA: 3:36 - loss: 3.4702 - acc: 0.168 - ETA: 3:35 - loss: 3.4546 - acc: 0.164 - ETA: 3:34 - loss: 3.4662 - acc: 0.174 - ETA: 3:32 - loss: 3.4173 - acc: 0.176 - ETA: 3:31 - loss: 3.4478 - acc: 0.173 - ETA: 3:29 - loss: 3.4687 - acc: 0.168 - ETA: 3:28 - loss: 3.4479 - acc: 0.172 - ETA: 3:26 - loss: 3.4704 - acc: 0.172 - ETA: 3:25 - loss: 3.4663 - acc: 0.165 - ETA: 3:23 - loss: 3.4811 - acc: 0.165 - ETA: 3:22 - loss: 3.4719 - acc: 0.170 - ETA: 3:20 - loss: 3.4583 - acc: 0.169 - ETA: 3:18 - loss: 3.4539 - acc: 0.166 - ETA: 3:17 - loss: 3.4658 - acc: 0.164 - ETA: 3:15 - loss: 3.4754 - acc: 0.160 - ETA: 3:13 - loss: 3.4825 - acc: 0.156 - ETA: 3:11 - loss: 3.4900 - acc: 0.156 - ETA: 3:10 - loss: 3.4902 - acc: 0.154 - ETA: 3:08 - loss: 3.4916 - acc: 0.154 - ETA: 3:07 - loss: 3.4900 - acc: 0.151 - ETA: 3:05 - loss: 3.4889 - acc: 0.150 - ETA: 3:03 - loss: 3.4974 - acc: 0.148 - ETA: 3:02 - loss: 3.5000 - acc: 0.147 - ETA: 3:00 - loss: 3.5049 - acc: 0.147 - ETA: 2:59 - loss: 3.5144 - acc: 0.145 - ETA: 2:57 - loss: 3.5092 - acc: 0.145 - ETA: 2:55 - loss: 3.5182 - acc: 0.146 - ETA: 2:53 - loss: 3.5158 - acc: 0.147 - ETA: 2:51 - loss: 3.5145 - acc: 0.147 - ETA: 2:49 - loss: 3.5149 - acc: 0.146 - ETA: 2:48 - loss: 3.5198 - acc: 0.145 - ETA: 2:46 - loss: 3.5181 - acc: 0.145 - ETA: 2:44 - loss: 3.5188 - acc: 0.145 - ETA: 2:42 - loss: 3.5188 - acc: 0.147 - ETA: 2:41 - loss: 3.5159 - acc: 0.147 - ETA: 2:39 - loss: 3.5118 - acc: 0.149 - ETA: 2:36 - loss: 3.5090 - acc: 0.149 - ETA: 2:34 - loss: 3.5150 - acc: 0.149 - ETA: 2:32 - loss: 3.5140 - acc: 0.150 - ETA: 2:31 - loss: 3.5116 - acc: 0.151 - ETA: 2:29 - loss: 3.5190 - acc: 0.150 - ETA: 2:27 - loss: 3.5136 - acc: 0.152 - ETA: 2:25 - loss: 3.5128 - acc: 0.155 - ETA: 2:23 - loss: 3.5193 - acc: 0.154 - ETA: 2:22 - loss: 3.5200 - acc: 0.154 - ETA: 2:20 - loss: 3.5209 - acc: 0.153 - ETA: 2:18 - loss: 3.5191 - acc: 0.152 - ETA: 2:16 - loss: 3.5153 - acc: 0.153 - ETA: 2:15 - loss: 3.5140 - acc: 0.153 - ETA: 2:13 - loss: 3.5099 - acc: 0.155 - ETA: 2:11 - loss: 3.5012 - acc: 0.157 - ETA: 2:09 - loss: 3.4984 - acc: 0.157 - ETA: 2:08 - loss: 3.5001 - acc: 0.157 - ETA: 2:06 - loss: 3.5008 - acc: 0.157 - ETA: 2:04 - loss: 3.5076 - acc: 0.156 - ETA: 2:02 - loss: 3.5128 - acc: 0.155 - ETA: 2:00 - loss: 3.5092 - acc: 0.155 - ETA: 1:59 - loss: 3.5113 - acc: 0.154 - ETA: 1:57 - loss: 3.5091 - acc: 0.155 - ETA: 1:55 - loss: 3.5093 - acc: 0.154 - ETA: 1:53 - loss: 3.5129 - acc: 0.153 - ETA: 1:51 - loss: 3.5106 - acc: 0.154 - ETA: 1:50 - loss: 3.5064 - acc: 0.154 - ETA: 1:48 - loss: 3.5116 - acc: 0.154 - ETA: 1:46 - loss: 3.5087 - acc: 0.154 - ETA: 1:44 - loss: 3.5101 - acc: 0.154 - ETA: 1:43 - loss: 3.5168 - acc: 0.155 - ETA: 1:41 - loss: 3.5191 - acc: 0.154 - ETA: 1:39 - loss: 3.5172 - acc: 0.153 - ETA: 1:37 - loss: 3.5150 - acc: 0.153 - ETA: 1:36 - loss: 3.5141 - acc: 0.153 - ETA: 1:34 - loss: 3.5177 - acc: 0.153 - ETA: 1:32 - loss: 3.5217 - acc: 0.153 - ETA: 1:30 - loss: 3.5218 - acc: 0.153 - ETA: 1:28 - loss: 3.5209 - acc: 0.154 - ETA: 1:27 - loss: 3.5223 - acc: 0.153 - ETA: 1:25 - loss: 3.5195 - acc: 0.154 - ETA: 1:23 - loss: 3.5201 - acc: 0.154 - ETA: 1:21 - loss: 3.5209 - acc: 0.155 - ETA: 1:20 - loss: 3.5194 - acc: 0.154 - ETA: 1:18 - loss: 3.5182 - acc: 0.154 - ETA: 1:16 - loss: 3.5176 - acc: 0.155 - ETA: 1:14 - loss: 3.5104 - acc: 0.155 - ETA: 1:12 - loss: 3.5145 - acc: 0.154 - ETA: 1:11 - loss: 3.5164 - acc: 0.156 - ETA: 1:09 - loss: 3.5172 - acc: 0.156 - ETA: 1:07 - loss: 3.5163 - acc: 0.156 - ETA: 1:05 - loss: 3.5201 - acc: 0.156 - ETA: 1:04 - loss: 3.5201 - acc: 0.155 - ETA: 1:02 - loss: 3.5214 - acc: 0.155 - ETA: 1:00 - loss: 3.5184 - acc: 0.156 - ETA: 58s - loss: 3.5215 - acc: 0.155 - ETA: 56s - loss: 3.5222 - acc: 0.15 - ETA: 55s - loss: 3.5229 - acc: 0.15 - ETA: 53s - loss: 3.5210 - acc: 0.15 - ETA: 51s - loss: 3.5225 - acc: 0.15 - ETA: 49s - loss: 3.5226 - acc: 0.15 - ETA: 48s - loss: 3.5270 - acc: 0.15 - ETA: 46s - loss: 3.5311 - acc: 0.15 - ETA: 44s - loss: 3.5268 - acc: 0.15 - ETA: 42s - loss: 3.5210 - acc: 0.15 - ETA: 40s - loss: 3.5181 - acc: 0.15 - ETA: 39s - loss: 3.5162 - acc: 0.15 - ETA: 37s - loss: 3.5206 - acc: 0.15 - ETA: 35s - loss: 3.5195 - acc: 0.15 - ETA: 33s - loss: 3.5204 - acc: 0.15 - ETA: 32s - loss: 3.5199 - acc: 0.15 - ETA: 30s - loss: 3.5211 - acc: 0.15 - ETA: 28s - loss: 3.5175 - acc: 0.15 - ETA: 26s - loss: 3.5167 - acc: 0.15 - ETA: 24s - loss: 3.5165 - acc: 0.15 - ETA: 23s - loss: 3.5151 - acc: 0.15 - ETA: 21s - loss: 3.5135 - acc: 0.15 - ETA: 19s - loss: 3.5184 - acc: 0.15 - ETA: 17s - loss: 3.5175 - acc: 0.15 - ETA: 16s - loss: 3.5154 - acc: 0.15 - ETA: 14s - loss: 3.5173 - acc: 0.15 - ETA: 12s - loss: 3.5191 - acc: 0.15 - ETA: 10s - loss: 3.5157 - acc: 0.15 - ETA: 8s - loss: 3.5140 - acc: 0.1535 - ETA: 7s - loss: 3.5168 - acc: 0.152 - ETA: 5s - loss: 3.5191 - acc: 0.152 - ETA: 3s - loss: 3.5208 - acc: 0.152 - ETA: 1s - loss: 3.5185 - acc: 0.152 - 246s 2s/step - loss: 3.5168 - acc: 0.1519 - val_loss: 3.5939 - val_acc: 0.1485\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.65587 to 3.59394, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - ETA: 3:43 - loss: 3.4307 - acc: 0.100 - ETA: 3:43 - loss: 3.3695 - acc: 0.140 - ETA: 3:39 - loss: 3.5056 - acc: 0.120 - ETA: 3:38 - loss: 3.4306 - acc: 0.155 - ETA: 3:38 - loss: 3.4523 - acc: 0.156 - ETA: 3:36 - loss: 3.4661 - acc: 0.173 - ETA: 3:35 - loss: 3.5180 - acc: 0.180 - ETA: 3:35 - loss: 3.5243 - acc: 0.182 - ETA: 3:34 - loss: 3.5137 - acc: 0.175 - ETA: 3:33 - loss: 3.4900 - acc: 0.184 - ETA: 3:32 - loss: 3.4687 - acc: 0.178 - ETA: 3:31 - loss: 3.4565 - acc: 0.173 - ETA: 3:30 - loss: 3.4403 - acc: 0.170 - ETA: 3:29 - loss: 3.4210 - acc: 0.170 - ETA: 3:27 - loss: 3.4270 - acc: 0.172 - ETA: 3:26 - loss: 3.4567 - acc: 0.166 - ETA: 3:24 - loss: 3.4495 - acc: 0.167 - ETA: 3:23 - loss: 3.4411 - acc: 0.167 - ETA: 3:21 - loss: 3.4258 - acc: 0.166 - ETA: 3:20 - loss: 3.4188 - acc: 0.164 - ETA: 3:18 - loss: 3.4357 - acc: 0.161 - ETA: 3:16 - loss: 3.4454 - acc: 0.158 - ETA: 3:15 - loss: 3.4629 - acc: 0.156 - ETA: 3:13 - loss: 3.4785 - acc: 0.156 - ETA: 3:11 - loss: 3.4793 - acc: 0.154 - ETA: 3:10 - loss: 3.4773 - acc: 0.155 - ETA: 3:08 - loss: 3.4792 - acc: 0.155 - ETA: 3:06 - loss: 3.4816 - acc: 0.155 - ETA: 3:04 - loss: 3.4741 - acc: 0.155 - ETA: 3:03 - loss: 3.4819 - acc: 0.153 - ETA: 3:01 - loss: 3.4812 - acc: 0.155 - ETA: 2:59 - loss: 3.4819 - acc: 0.155 - ETA: 2:58 - loss: 3.4762 - acc: 0.155 - ETA: 2:56 - loss: 3.4915 - acc: 0.151 - ETA: 2:54 - loss: 3.4956 - acc: 0.153 - ETA: 2:52 - loss: 3.4885 - acc: 0.154 - ETA: 2:50 - loss: 3.4901 - acc: 0.152 - ETA: 2:49 - loss: 3.4893 - acc: 0.152 - ETA: 2:47 - loss: 3.4936 - acc: 0.151 - ETA: 2:45 - loss: 3.4950 - acc: 0.151 - ETA: 2:43 - loss: 3.4901 - acc: 0.151 - ETA: 2:41 - loss: 3.4910 - acc: 0.151 - ETA: 2:40 - loss: 3.5020 - acc: 0.149 - ETA: 2:38 - loss: 3.4967 - acc: 0.150 - ETA: 2:36 - loss: 3.4949 - acc: 0.150 - ETA: 2:34 - loss: 3.4943 - acc: 0.149 - ETA: 2:32 - loss: 3.4922 - acc: 0.150 - ETA: 2:31 - loss: 3.4909 - acc: 0.149 - ETA: 2:29 - loss: 3.4946 - acc: 0.149 - ETA: 2:27 - loss: 3.5005 - acc: 0.147 - ETA: 2:25 - loss: 3.5017 - acc: 0.146 - ETA: 2:24 - loss: 3.4981 - acc: 0.147 - ETA: 2:22 - loss: 3.4969 - acc: 0.147 - ETA: 2:20 - loss: 3.4950 - acc: 0.146 - ETA: 2:18 - loss: 3.5002 - acc: 0.146 - ETA: 2:16 - loss: 3.4996 - acc: 0.146 - ETA: 2:15 - loss: 3.4960 - acc: 0.146 - ETA: 2:13 - loss: 3.4946 - acc: 0.145 - ETA: 2:11 - loss: 3.4928 - acc: 0.145 - ETA: 2:09 - loss: 3.4885 - acc: 0.146 - ETA: 2:08 - loss: 3.4933 - acc: 0.146 - ETA: 2:06 - loss: 3.4881 - acc: 0.148 - ETA: 2:04 - loss: 3.4870 - acc: 0.149 - ETA: 2:02 - loss: 3.4901 - acc: 0.149 - ETA: 2:00 - loss: 3.4927 - acc: 0.149 - ETA: 1:59 - loss: 3.4909 - acc: 0.149 - ETA: 1:57 - loss: 3.4847 - acc: 0.150 - ETA: 1:55 - loss: 3.4806 - acc: 0.151 - ETA: 1:53 - loss: 3.4813 - acc: 0.151 - ETA: 1:52 - loss: 3.4788 - acc: 0.151 - ETA: 1:50 - loss: 3.4782 - acc: 0.151 - ETA: 1:48 - loss: 3.4800 - acc: 0.150 - ETA: 1:46 - loss: 3.4804 - acc: 0.150 - ETA: 1:45 - loss: 3.4850 - acc: 0.150 - ETA: 1:43 - loss: 3.4827 - acc: 0.150 - ETA: 1:41 - loss: 3.4848 - acc: 0.150 - ETA: 1:39 - loss: 3.4838 - acc: 0.149 - ETA: 1:37 - loss: 3.4822 - acc: 0.150 - ETA: 1:36 - loss: 3.4781 - acc: 0.151 - ETA: 1:34 - loss: 3.4769 - acc: 0.151 - ETA: 1:32 - loss: 3.4740 - acc: 0.153 - ETA: 1:30 - loss: 3.4729 - acc: 0.154 - ETA: 1:28 - loss: 3.4685 - acc: 0.155 - ETA: 1:27 - loss: 3.4700 - acc: 0.154 - ETA: 1:25 - loss: 3.4699 - acc: 0.153 - ETA: 1:23 - loss: 3.4774 - acc: 0.152 - ETA: 1:21 - loss: 3.4754 - acc: 0.153 - ETA: 1:20 - loss: 3.4731 - acc: 0.154 - ETA: 1:18 - loss: 3.4717 - acc: 0.155 - ETA: 1:16 - loss: 3.4737 - acc: 0.154 - ETA: 1:14 - loss: 3.4736 - acc: 0.154 - ETA: 1:12 - loss: 3.4725 - acc: 0.154 - ETA: 1:11 - loss: 3.4732 - acc: 0.154 - ETA: 1:09 - loss: 3.4708 - acc: 0.154 - ETA: 1:07 - loss: 3.4700 - acc: 0.154 - ETA: 1:05 - loss: 3.4706 - acc: 0.154 - ETA: 1:04 - loss: 3.4717 - acc: 0.154 - ETA: 1:02 - loss: 3.4731 - acc: 0.154 - ETA: 1:00 - loss: 3.4745 - acc: 0.154 - ETA: 58s - loss: 3.4753 - acc: 0.155 - ETA: 56s - loss: 3.4745 - acc: 0.15 - ETA: 55s - loss: 3.4734 - acc: 0.15 - ETA: 53s - loss: 3.4697 - acc: 0.15 - ETA: 51s - loss: 3.4702 - acc: 0.15 - ETA: 49s - loss: 3.4698 - acc: 0.15 - ETA: 48s - loss: 3.4708 - acc: 0.15 - ETA: 46s - loss: 3.4729 - acc: 0.15 - ETA: 44s - loss: 3.4737 - acc: 0.15 - ETA: 42s - loss: 3.4747 - acc: 0.15 - ETA: 40s - loss: 3.4747 - acc: 0.15 - ETA: 39s - loss: 3.4732 - acc: 0.15 - ETA: 37s - loss: 3.4754 - acc: 0.15 - ETA: 35s - loss: 3.4783 - acc: 0.15 - ETA: 33s - loss: 3.4816 - acc: 0.15 - ETA: 31s - loss: 3.4856 - acc: 0.15 - ETA: 30s - loss: 3.4835 - acc: 0.15 - ETA: 28s - loss: 3.4846 - acc: 0.15 - ETA: 26s - loss: 3.4853 - acc: 0.15 - ETA: 24s - loss: 3.4870 - acc: 0.15 - ETA: 23s - loss: 3.4852 - acc: 0.15 - ETA: 21s - loss: 3.4811 - acc: 0.15 - ETA: 19s - loss: 3.4833 - acc: 0.15 - ETA: 17s - loss: 3.4825 - acc: 0.15 - ETA: 15s - loss: 3.4841 - acc: 0.15 - ETA: 14s - loss: 3.4840 - acc: 0.15 - ETA: 12s - loss: 3.4837 - acc: 0.15 - ETA: 10s - loss: 3.4846 - acc: 0.15 - ETA: 8s - loss: 3.4831 - acc: 0.1530 - ETA: 7s - loss: 3.4819 - acc: 0.152 - ETA: 5s - loss: 3.4802 - acc: 0.153 - ETA: 3s - loss: 3.4766 - acc: 0.153 - ETA: 1s - loss: 3.4768 - acc: 0.153 - 247s 2s/step - loss: 3.4768 - acc: 0.1537 - val_loss: 3.5957 - val_acc: 0.1293\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 4:10 - loss: 3.5881 - acc: 0.100 - ETA: 4:03 - loss: 3.5094 - acc: 0.120 - ETA: 3:59 - loss: 3.5405 - acc: 0.133 - ETA: 3:57 - loss: 3.5186 - acc: 0.150 - ETA: 3:57 - loss: 3.5138 - acc: 0.140 - ETA: 3:55 - loss: 3.5211 - acc: 0.133 - ETA: 3:52 - loss: 3.5038 - acc: 0.131 - ETA: 3:51 - loss: 3.4806 - acc: 0.140 - ETA: 3:50 - loss: 3.4532 - acc: 0.146 - ETA: 3:49 - loss: 3.4634 - acc: 0.152 - ETA: 3:48 - loss: 3.4720 - acc: 0.145 - ETA: 3:46 - loss: 3.4661 - acc: 0.146 - ETA: 3:45 - loss: 3.4541 - acc: 0.149 - ETA: 3:43 - loss: 3.4663 - acc: 0.147 - ETA: 3:42 - loss: 3.4514 - acc: 0.152 - ETA: 3:40 - loss: 3.4341 - acc: 0.155 - ETA: 3:38 - loss: 3.4326 - acc: 0.161 - ETA: 3:37 - loss: 3.4290 - acc: 0.160 - ETA: 3:35 - loss: 3.4435 - acc: 0.156 - ETA: 3:33 - loss: 3.4481 - acc: 0.156 - ETA: 3:32 - loss: 3.4320 - acc: 0.159 - ETA: 3:30 - loss: 3.4583 - acc: 0.157 - ETA: 3:28 - loss: 3.4586 - acc: 0.156 - ETA: 3:26 - loss: 3.4560 - acc: 0.158 - ETA: 3:25 - loss: 3.4632 - acc: 0.156 - ETA: 3:23 - loss: 3.4663 - acc: 0.158 - ETA: 3:21 - loss: 3.4577 - acc: 0.158 - ETA: 3:19 - loss: 3.4528 - acc: 0.160 - ETA: 3:18 - loss: 3.4512 - acc: 0.160 - ETA: 3:16 - loss: 3.4459 - acc: 0.162 - ETA: 3:14 - loss: 3.4541 - acc: 0.161 - ETA: 3:12 - loss: 3.4600 - acc: 0.160 - ETA: 3:10 - loss: 3.4646 - acc: 0.159 - ETA: 3:08 - loss: 3.4559 - acc: 0.159 - ETA: 3:06 - loss: 3.4530 - acc: 0.159 - ETA: 3:05 - loss: 3.4454 - acc: 0.160 - ETA: 3:03 - loss: 3.4442 - acc: 0.158 - ETA: 3:01 - loss: 3.4411 - acc: 0.159 - ETA: 2:59 - loss: 3.4486 - acc: 0.160 - ETA: 2:57 - loss: 3.4548 - acc: 0.160 - ETA: 2:55 - loss: 3.4543 - acc: 0.159 - ETA: 2:54 - loss: 3.4570 - acc: 0.159 - ETA: 2:52 - loss: 3.4541 - acc: 0.159 - ETA: 2:50 - loss: 3.4565 - acc: 0.160 - ETA: 2:48 - loss: 3.4501 - acc: 0.160 - ETA: 2:46 - loss: 3.4500 - acc: 0.160 - ETA: 2:44 - loss: 3.4585 - acc: 0.159 - ETA: 2:42 - loss: 3.4569 - acc: 0.159 - ETA: 2:40 - loss: 3.4576 - acc: 0.160 - ETA: 2:38 - loss: 3.4560 - acc: 0.160 - ETA: 2:37 - loss: 3.4585 - acc: 0.160 - ETA: 2:35 - loss: 3.4569 - acc: 0.161 - ETA: 2:33 - loss: 3.4578 - acc: 0.160 - ETA: 2:31 - loss: 3.4591 - acc: 0.159 - ETA: 2:29 - loss: 3.4638 - acc: 0.160 - ETA: 2:27 - loss: 3.4647 - acc: 0.158 - ETA: 2:25 - loss: 3.4588 - acc: 0.161 - ETA: 2:23 - loss: 3.4537 - acc: 0.161 - ETA: 2:21 - loss: 3.4529 - acc: 0.160 - ETA: 2:19 - loss: 3.4543 - acc: 0.159 - ETA: 2:18 - loss: 3.4617 - acc: 0.158 - ETA: 2:16 - loss: 3.4594 - acc: 0.159 - ETA: 2:14 - loss: 3.4564 - acc: 0.159 - ETA: 2:12 - loss: 3.4528 - acc: 0.160 - ETA: 2:10 - loss: 3.4585 - acc: 0.158 - ETA: 2:08 - loss: 3.4531 - acc: 0.161 - ETA: 2:06 - loss: 3.4524 - acc: 0.161 - ETA: 2:04 - loss: 3.4547 - acc: 0.161 - ETA: 2:02 - loss: 3.4552 - acc: 0.160 - ETA: 2:00 - loss: 3.4574 - acc: 0.161 - ETA: 1:59 - loss: 3.4583 - acc: 0.161 - ETA: 1:57 - loss: 3.4595 - acc: 0.160 - ETA: 1:55 - loss: 3.4572 - acc: 0.161 - ETA: 1:53 - loss: 3.4568 - acc: 0.160 - ETA: 1:51 - loss: 3.4544 - acc: 0.161 - ETA: 1:49 - loss: 3.4544 - acc: 0.161 - ETA: 1:47 - loss: 3.4559 - acc: 0.160 - ETA: 1:45 - loss: 3.4544 - acc: 0.161 - ETA: 1:43 - loss: 3.4529 - acc: 0.161 - ETA: 1:41 - loss: 3.4505 - acc: 0.161 - ETA: 1:39 - loss: 3.4459 - acc: 0.161 - ETA: 1:37 - loss: 3.4405 - acc: 0.163 - ETA: 1:36 - loss: 3.4435 - acc: 0.162 - ETA: 1:34 - loss: 3.4433 - acc: 0.161 - ETA: 1:32 - loss: 3.4468 - acc: 0.161 - ETA: 1:30 - loss: 3.4481 - acc: 0.160 - ETA: 1:28 - loss: 3.4483 - acc: 0.160 - ETA: 1:26 - loss: 3.4494 - acc: 0.159 - ETA: 1:24 - loss: 3.4482 - acc: 0.158 - ETA: 1:22 - loss: 3.4426 - acc: 0.159 - ETA: 1:20 - loss: 3.4425 - acc: 0.159 - ETA: 1:18 - loss: 3.4413 - acc: 0.159 - ETA: 1:17 - loss: 3.4374 - acc: 0.160 - ETA: 1:15 - loss: 3.4382 - acc: 0.160 - ETA: 1:13 - loss: 3.4398 - acc: 0.160 - ETA: 1:11 - loss: 3.4409 - acc: 0.160 - ETA: 1:09 - loss: 3.4408 - acc: 0.159 - ETA: 1:07 - loss: 3.4382 - acc: 0.160 - ETA: 1:05 - loss: 3.4388 - acc: 0.160 - ETA: 1:03 - loss: 3.4404 - acc: 0.160 - ETA: 1:01 - loss: 3.4395 - acc: 0.161 - ETA: 59s - loss: 3.4397 - acc: 0.160 - ETA: 57s - loss: 3.4401 - acc: 0.16 - ETA: 55s - loss: 3.4423 - acc: 0.15 - ETA: 53s - loss: 3.4433 - acc: 0.15 - ETA: 52s - loss: 3.4451 - acc: 0.15 - ETA: 50s - loss: 3.4428 - acc: 0.16 - ETA: 48s - loss: 3.4430 - acc: 0.16 - ETA: 46s - loss: 3.4430 - acc: 0.16 - ETA: 44s - loss: 3.4421 - acc: 0.16 - ETA: 42s - loss: 3.4400 - acc: 0.16 - ETA: 40s - loss: 3.4401 - acc: 0.16 - ETA: 38s - loss: 3.4416 - acc: 0.16 - ETA: 36s - loss: 3.4407 - acc: 0.16 - ETA: 34s - loss: 3.4403 - acc: 0.16 - ETA: 32s - loss: 3.4395 - acc: 0.16 - ETA: 30s - loss: 3.4378 - acc: 0.16 - ETA: 28s - loss: 3.4382 - acc: 0.16 - ETA: 26s - loss: 3.4378 - acc: 0.16 - ETA: 24s - loss: 3.4352 - acc: 0.16 - ETA: 23s - loss: 3.4352 - acc: 0.16 - ETA: 21s - loss: 3.4339 - acc: 0.16 - ETA: 19s - loss: 3.4366 - acc: 0.15 - ETA: 17s - loss: 3.4380 - acc: 0.15 - ETA: 15s - loss: 3.4413 - acc: 0.15 - ETA: 13s - loss: 3.4430 - acc: 0.15 - ETA: 11s - loss: 3.4414 - acc: 0.15 - ETA: 9s - loss: 3.4389 - acc: 0.1599 - ETA: 7s - loss: 3.4386 - acc: 0.160 - ETA: 5s - loss: 3.4390 - acc: 0.159 - ETA: 3s - loss: 3.4397 - acc: 0.159 - ETA: 1s - loss: 3.4377 - acc: 0.159 - 265s 2s/step - loss: 3.4391 - acc: 0.1590 - val_loss: 3.6733 - val_acc: 0.1174\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - ETA: 4:05 - loss: 3.4195 - acc: 0.120 - ETA: 4:03 - loss: 3.3709 - acc: 0.120 - ETA: 4:03 - loss: 3.3824 - acc: 0.133 - ETA: 4:00 - loss: 3.3601 - acc: 0.140 - ETA: 3:59 - loss: 3.3791 - acc: 0.140 - ETA: 3:57 - loss: 3.4102 - acc: 0.143 - ETA: 3:56 - loss: 3.4062 - acc: 0.140 - ETA: 3:52 - loss: 3.3971 - acc: 0.142 - ETA: 3:51 - loss: 3.4212 - acc: 0.135 - ETA: 3:49 - loss: 3.4334 - acc: 0.134 - ETA: 3:48 - loss: 3.4089 - acc: 0.132 - ETA: 3:46 - loss: 3.3987 - acc: 0.133 - ETA: 3:45 - loss: 3.4159 - acc: 0.129 - ETA: 3:44 - loss: 3.4484 - acc: 0.131 - ETA: 3:42 - loss: 3.4553 - acc: 0.134 - ETA: 3:40 - loss: 3.4406 - acc: 0.142 - ETA: 3:39 - loss: 3.4382 - acc: 0.147 - ETA: 3:37 - loss: 3.4362 - acc: 0.148 - ETA: 3:35 - loss: 3.4481 - acc: 0.144 - ETA: 3:33 - loss: 3.4424 - acc: 0.146 - ETA: 3:31 - loss: 3.4401 - acc: 0.149 - ETA: 3:30 - loss: 3.4314 - acc: 0.152 - ETA: 3:28 - loss: 3.4240 - acc: 0.155 - ETA: 3:26 - loss: 3.4299 - acc: 0.155 - ETA: 3:24 - loss: 3.4299 - acc: 0.155 - ETA: 3:22 - loss: 3.4273 - acc: 0.157 - ETA: 3:20 - loss: 3.4331 - acc: 0.156 - ETA: 3:18 - loss: 3.4295 - acc: 0.157 - ETA: 3:16 - loss: 3.4294 - acc: 0.156 - ETA: 3:15 - loss: 3.4307 - acc: 0.159 - ETA: 3:13 - loss: 3.4308 - acc: 0.162 - ETA: 3:11 - loss: 3.4312 - acc: 0.165 - ETA: 3:09 - loss: 3.4270 - acc: 0.167 - ETA: 3:07 - loss: 3.4185 - acc: 0.165 - ETA: 3:06 - loss: 3.4370 - acc: 0.163 - ETA: 3:04 - loss: 3.4325 - acc: 0.164 - ETA: 3:02 - loss: 3.4324 - acc: 0.165 - ETA: 3:00 - loss: 3.4317 - acc: 0.165 - ETA: 2:58 - loss: 3.4361 - acc: 0.166 - ETA: 2:56 - loss: 3.4480 - acc: 0.164 - ETA: 2:54 - loss: 3.4437 - acc: 0.163 - ETA: 2:53 - loss: 3.4449 - acc: 0.162 - ETA: 2:51 - loss: 3.4372 - acc: 0.163 - ETA: 2:49 - loss: 3.4320 - acc: 0.163 - ETA: 2:47 - loss: 3.4272 - acc: 0.164 - ETA: 2:45 - loss: 3.4364 - acc: 0.163 - ETA: 2:43 - loss: 3.4370 - acc: 0.162 - ETA: 2:41 - loss: 3.4388 - acc: 0.161 - ETA: 2:40 - loss: 3.4438 - acc: 0.161 - ETA: 2:38 - loss: 3.4447 - acc: 0.162 - ETA: 2:36 - loss: 3.4401 - acc: 0.163 - ETA: 2:34 - loss: 3.4355 - acc: 0.166 - ETA: 2:32 - loss: 3.4328 - acc: 0.166 - ETA: 2:30 - loss: 3.4354 - acc: 0.166 - ETA: 2:28 - loss: 3.4410 - acc: 0.165 - ETA: 2:26 - loss: 3.4425 - acc: 0.163 - ETA: 2:25 - loss: 3.4410 - acc: 0.164 - ETA: 2:23 - loss: 3.4438 - acc: 0.162 - ETA: 2:21 - loss: 3.4390 - acc: 0.162 - ETA: 2:19 - loss: 3.4356 - acc: 0.162 - ETA: 2:17 - loss: 3.4392 - acc: 0.161 - ETA: 2:15 - loss: 3.4392 - acc: 0.161 - ETA: 2:13 - loss: 3.4385 - acc: 0.161 - ETA: 2:12 - loss: 3.4379 - acc: 0.161 - ETA: 2:10 - loss: 3.4420 - acc: 0.161 - ETA: 2:08 - loss: 3.4468 - acc: 0.161 - ETA: 2:06 - loss: 3.4462 - acc: 0.160 - ETA: 2:04 - loss: 3.4426 - acc: 0.162 - ETA: 2:02 - loss: 3.4427 - acc: 0.162 - ETA: 2:00 - loss: 3.4393 - acc: 0.162 - ETA: 1:58 - loss: 3.4357 - acc: 0.163 - ETA: 1:56 - loss: 3.4369 - acc: 0.162 - ETA: 1:54 - loss: 3.4357 - acc: 0.161 - ETA: 1:53 - loss: 3.4384 - acc: 0.161 - ETA: 1:51 - loss: 3.4394 - acc: 0.160 - ETA: 1:49 - loss: 3.4413 - acc: 0.160 - ETA: 1:47 - loss: 3.4425 - acc: 0.160 - ETA: 1:45 - loss: 3.4449 - acc: 0.160 - ETA: 1:43 - loss: 3.4493 - acc: 0.159 - ETA: 1:41 - loss: 3.4462 - acc: 0.160 - ETA: 1:39 - loss: 3.4446 - acc: 0.160 - ETA: 1:37 - loss: 3.4417 - acc: 0.161 - ETA: 1:35 - loss: 3.4397 - acc: 0.163 - ETA: 1:33 - loss: 3.4363 - acc: 0.163 - ETA: 1:31 - loss: 3.4352 - acc: 0.163 - ETA: 1:30 - loss: 3.4302 - acc: 0.164 - ETA: 1:28 - loss: 3.4297 - acc: 0.165 - ETA: 1:26 - loss: 3.4324 - acc: 0.164 - ETA: 1:24 - loss: 3.4327 - acc: 0.164 - ETA: 1:22 - loss: 3.4333 - acc: 0.164 - ETA: 1:20 - loss: 3.4336 - acc: 0.164 - ETA: 1:18 - loss: 3.4319 - acc: 0.165 - ETA: 1:16 - loss: 3.4338 - acc: 0.166 - ETA: 1:14 - loss: 3.4308 - acc: 0.167 - ETA: 1:12 - loss: 3.4297 - acc: 0.167 - ETA: 1:10 - loss: 3.4245 - acc: 0.168 - ETA: 1:09 - loss: 3.4270 - acc: 0.168 - ETA: 1:07 - loss: 3.4325 - acc: 0.168 - ETA: 1:05 - loss: 3.4334 - acc: 0.167 - ETA: 1:03 - loss: 3.4333 - acc: 0.166 - ETA: 1:01 - loss: 3.4335 - acc: 0.167 - ETA: 59s - loss: 3.4342 - acc: 0.166 - ETA: 57s - loss: 3.4302 - acc: 0.16 - ETA: 55s - loss: 3.4303 - acc: 0.16 - ETA: 53s - loss: 3.4303 - acc: 0.16 - ETA: 51s - loss: 3.4296 - acc: 0.16 - ETA: 49s - loss: 3.4270 - acc: 0.16 - ETA: 48s - loss: 3.4300 - acc: 0.16 - ETA: 46s - loss: 3.4315 - acc: 0.16 - ETA: 44s - loss: 3.4316 - acc: 0.16 - ETA: 42s - loss: 3.4315 - acc: 0.16 - ETA: 40s - loss: 3.4310 - acc: 0.16 - ETA: 38s - loss: 3.4301 - acc: 0.16 - ETA: 36s - loss: 3.4259 - acc: 0.16 - ETA: 34s - loss: 3.4252 - acc: 0.16 - ETA: 32s - loss: 3.4249 - acc: 0.16 - ETA: 30s - loss: 3.4247 - acc: 0.16 - ETA: 28s - loss: 3.4224 - acc: 0.16 - ETA: 26s - loss: 3.4257 - acc: 0.16 - ETA: 24s - loss: 3.4250 - acc: 0.16 - ETA: 22s - loss: 3.4222 - acc: 0.16 - ETA: 21s - loss: 3.4175 - acc: 0.16 - ETA: 19s - loss: 3.4208 - acc: 0.16 - ETA: 17s - loss: 3.4216 - acc: 0.16 - ETA: 15s - loss: 3.4218 - acc: 0.16 - ETA: 13s - loss: 3.4211 - acc: 0.16 - ETA: 11s - loss: 3.4231 - acc: 0.16 - ETA: 9s - loss: 3.4244 - acc: 0.1665 - ETA: 7s - loss: 3.4286 - acc: 0.165 - ETA: 5s - loss: 3.4296 - acc: 0.166 - ETA: 3s - loss: 3.4288 - acc: 0.166 - ETA: 1s - loss: 3.4283 - acc: 0.166 - 265s 2s/step - loss: 3.4291 - acc: 0.1659 - val_loss: 3.4776 - val_acc: 0.1353\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.59394 to 3.47763, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3fc7386710>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "batch_size=50\n",
    "\n",
    "# augmented image generator\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2, \n",
    "    horizontal_flip=True) \n",
    "\n",
    "# fit augmented image generator on data\n",
    "datagen.fit(train_tensors)\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
    "                    epochs=epochs, callbacks=[checkpointer], verbose=1,\n",
    "                   validation_data=(valid_tensors, valid_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 16.1483%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_11  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 1:01:40 - loss: 14.5766 - acc: 0.0000e+0 - ETA: 10:10 - loss: 15.0410 - acc: 0.0083      - ETA: 5:01 - loss: 14.6100 - acc: 0.012 - ETA: 3:18 - loss: 14.5841 - acc: 0.01 - ETA: 2:26 - loss: 14.4060 - acc: 0.02 - ETA: 1:55 - loss: 14.3660 - acc: 0.02 - ETA: 1:37 - loss: 14.2833 - acc: 0.02 - ETA: 1:22 - loss: 14.1196 - acc: 0.02 - ETA: 1:10 - loss: 14.0775 - acc: 0.02 - ETA: 1:01 - loss: 13.9369 - acc: 0.03 - ETA: 54s - loss: 13.8864 - acc: 0.0331 - ETA: 48s - loss: 13.8541 - acc: 0.033 - ETA: 43s - loss: 13.8086 - acc: 0.033 - ETA: 39s - loss: 13.7965 - acc: 0.035 - ETA: 35s - loss: 13.7515 - acc: 0.035 - ETA: 32s - loss: 13.6461 - acc: 0.039 - ETA: 30s - loss: 13.5563 - acc: 0.045 - ETA: 27s - loss: 13.5166 - acc: 0.047 - ETA: 25s - loss: 13.5088 - acc: 0.047 - ETA: 23s - loss: 13.4410 - acc: 0.050 - ETA: 22s - loss: 13.3393 - acc: 0.054 - ETA: 20s - loss: 13.2880 - acc: 0.054 - ETA: 19s - loss: 13.2114 - acc: 0.056 - ETA: 17s - loss: 13.1403 - acc: 0.059 - ETA: 16s - loss: 13.0620 - acc: 0.064 - ETA: 15s - loss: 13.0051 - acc: 0.068 - ETA: 14s - loss: 12.9644 - acc: 0.071 - ETA: 13s - loss: 12.8880 - acc: 0.076 - ETA: 12s - loss: 12.8329 - acc: 0.080 - ETA: 11s - loss: 12.7882 - acc: 0.083 - ETA: 11s - loss: 12.7179 - acc: 0.087 - ETA: 10s - loss: 12.6686 - acc: 0.088 - ETA: 9s - loss: 12.6169 - acc: 0.091 - ETA: 9s - loss: 12.5790 - acc: 0.09 - ETA: 8s - loss: 12.5534 - acc: 0.09 - ETA: 7s - loss: 12.4889 - acc: 0.10 - ETA: 7s - loss: 12.4769 - acc: 0.10 - ETA: 6s - loss: 12.4199 - acc: 0.10 - ETA: 6s - loss: 12.3719 - acc: 0.10 - ETA: 5s - loss: 12.3557 - acc: 0.10 - ETA: 5s - loss: 12.3404 - acc: 0.10 - ETA: 4s - loss: 12.2813 - acc: 0.11 - ETA: 4s - loss: 12.2767 - acc: 0.11 - ETA: 4s - loss: 12.2199 - acc: 0.11 - ETA: 3s - loss: 12.1646 - acc: 0.11 - ETA: 3s - loss: 12.1353 - acc: 0.12 - ETA: 2s - loss: 12.0986 - acc: 0.12 - ETA: 2s - loss: 12.0353 - acc: 0.12 - ETA: 2s - loss: 12.0141 - acc: 0.12 - ETA: 1s - loss: 11.9914 - acc: 0.13 - ETA: 1s - loss: 11.9726 - acc: 0.13 - ETA: 1s - loss: 11.9347 - acc: 0.13 - ETA: 1s - loss: 11.9157 - acc: 0.13 - ETA: 0s - loss: 11.8828 - acc: 0.13 - ETA: 0s - loss: 11.8595 - acc: 0.13 - ETA: 0s - loss: 11.8318 - acc: 0.14 - 15s 2ms/step - loss: 11.7947 - acc: 0.1430 - val_loss: 10.2855 - val_acc: 0.2371\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.28550, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 12.0740 - acc: 0.25 - ETA: 3s - loss: 10.2495 - acc: 0.28 - ETA: 3s - loss: 9.9335 - acc: 0.3000 - ETA: 3s - loss: 10.0525 - acc: 0.28 - ETA: 3s - loss: 9.8795 - acc: 0.2913 - ETA: 3s - loss: 9.8718 - acc: 0.284 - ETA: 2s - loss: 9.8809 - acc: 0.289 - ETA: 2s - loss: 9.9106 - acc: 0.285 - ETA: 2s - loss: 9.8013 - acc: 0.293 - ETA: 2s - loss: 9.7508 - acc: 0.296 - ETA: 2s - loss: 9.8168 - acc: 0.293 - ETA: 2s - loss: 9.8493 - acc: 0.289 - ETA: 2s - loss: 9.8519 - acc: 0.287 - ETA: 2s - loss: 9.8425 - acc: 0.286 - ETA: 2s - loss: 9.8720 - acc: 0.288 - ETA: 2s - loss: 9.8302 - acc: 0.290 - ETA: 2s - loss: 9.7781 - acc: 0.295 - ETA: 2s - loss: 9.7030 - acc: 0.301 - ETA: 2s - loss: 9.7114 - acc: 0.299 - ETA: 2s - loss: 9.7397 - acc: 0.295 - ETA: 2s - loss: 9.7501 - acc: 0.294 - ETA: 2s - loss: 9.6853 - acc: 0.297 - ETA: 2s - loss: 9.7356 - acc: 0.294 - ETA: 2s - loss: 9.7325 - acc: 0.295 - ETA: 2s - loss: 9.7058 - acc: 0.297 - ETA: 1s - loss: 9.6426 - acc: 0.301 - ETA: 1s - loss: 9.6828 - acc: 0.299 - ETA: 1s - loss: 9.7015 - acc: 0.299 - ETA: 1s - loss: 9.7200 - acc: 0.298 - ETA: 1s - loss: 9.7453 - acc: 0.297 - ETA: 1s - loss: 9.7401 - acc: 0.298 - ETA: 1s - loss: 9.7199 - acc: 0.299 - ETA: 1s - loss: 9.7189 - acc: 0.299 - ETA: 1s - loss: 9.7390 - acc: 0.299 - ETA: 1s - loss: 9.7093 - acc: 0.300 - ETA: 1s - loss: 9.6930 - acc: 0.300 - ETA: 1s - loss: 9.6905 - acc: 0.299 - ETA: 1s - loss: 9.6827 - acc: 0.301 - ETA: 1s - loss: 9.7009 - acc: 0.300 - ETA: 1s - loss: 9.6899 - acc: 0.302 - ETA: 1s - loss: 9.6749 - acc: 0.303 - ETA: 0s - loss: 9.6429 - acc: 0.305 - ETA: 0s - loss: 9.6213 - acc: 0.306 - ETA: 0s - loss: 9.6151 - acc: 0.307 - ETA: 0s - loss: 9.6216 - acc: 0.307 - ETA: 0s - loss: 9.6193 - acc: 0.308 - ETA: 0s - loss: 9.5879 - acc: 0.309 - ETA: 0s - loss: 9.5962 - acc: 0.309 - ETA: 0s - loss: 9.6109 - acc: 0.308 - ETA: 0s - loss: 9.6122 - acc: 0.308 - ETA: 0s - loss: 9.6144 - acc: 0.308 - ETA: 0s - loss: 9.6165 - acc: 0.309 - ETA: 0s - loss: 9.6017 - acc: 0.309 - ETA: 0s - loss: 9.5833 - acc: 0.310 - ETA: 0s - loss: 9.5759 - acc: 0.311 - ETA: 0s - loss: 9.5872 - acc: 0.310 - ETA: 0s - loss: 9.5920 - acc: 0.310 - ETA: 0s - loss: 9.5967 - acc: 0.310 - 3s 490us/step - loss: 9.5762 - acc: 0.3112 - val_loss: 9.4420 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.28550 to 9.44205, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 6.1308 - acc: 0.350 - ETA: 3s - loss: 9.0487 - acc: 0.333 - ETA: 3s - loss: 8.6755 - acc: 0.377 - ETA: 3s - loss: 8.9326 - acc: 0.367 - ETA: 3s - loss: 9.2138 - acc: 0.356 - ETA: 3s - loss: 9.1449 - acc: 0.353 - ETA: 3s - loss: 8.8958 - acc: 0.371 - ETA: 2s - loss: 8.8811 - acc: 0.375 - ETA: 2s - loss: 9.0104 - acc: 0.367 - ETA: 2s - loss: 8.8739 - acc: 0.373 - ETA: 2s - loss: 9.0670 - acc: 0.361 - ETA: 2s - loss: 9.0586 - acc: 0.364 - ETA: 2s - loss: 9.0967 - acc: 0.358 - ETA: 2s - loss: 9.0464 - acc: 0.361 - ETA: 2s - loss: 9.0431 - acc: 0.363 - ETA: 2s - loss: 9.0813 - acc: 0.363 - ETA: 2s - loss: 9.2188 - acc: 0.354 - ETA: 2s - loss: 9.2179 - acc: 0.356 - ETA: 2s - loss: 9.2075 - acc: 0.355 - ETA: 2s - loss: 9.1549 - acc: 0.360 - ETA: 1s - loss: 9.2148 - acc: 0.356 - ETA: 1s - loss: 9.1538 - acc: 0.359 - ETA: 1s - loss: 9.0842 - acc: 0.363 - ETA: 1s - loss: 9.1237 - acc: 0.361 - ETA: 1s - loss: 9.1202 - acc: 0.362 - ETA: 1s - loss: 9.0703 - acc: 0.364 - ETA: 1s - loss: 9.0718 - acc: 0.365 - ETA: 1s - loss: 9.0836 - acc: 0.364 - ETA: 1s - loss: 9.0475 - acc: 0.365 - ETA: 1s - loss: 9.0535 - acc: 0.364 - ETA: 1s - loss: 9.0728 - acc: 0.363 - ETA: 1s - loss: 9.0835 - acc: 0.363 - ETA: 1s - loss: 9.0724 - acc: 0.364 - ETA: 1s - loss: 9.0481 - acc: 0.366 - ETA: 1s - loss: 9.0389 - acc: 0.366 - ETA: 1s - loss: 9.0194 - acc: 0.367 - ETA: 1s - loss: 9.0127 - acc: 0.368 - ETA: 1s - loss: 8.9765 - acc: 0.371 - ETA: 1s - loss: 9.0038 - acc: 0.370 - ETA: 0s - loss: 8.9944 - acc: 0.370 - ETA: 0s - loss: 8.9708 - acc: 0.371 - ETA: 0s - loss: 8.9779 - acc: 0.371 - ETA: 0s - loss: 8.9531 - acc: 0.372 - ETA: 0s - loss: 8.9645 - acc: 0.372 - ETA: 0s - loss: 8.9415 - acc: 0.373 - ETA: 0s - loss: 8.9386 - acc: 0.374 - ETA: 0s - loss: 8.9550 - acc: 0.373 - ETA: 0s - loss: 8.9552 - acc: 0.373 - ETA: 0s - loss: 8.9781 - acc: 0.372 - ETA: 0s - loss: 8.9557 - acc: 0.373 - ETA: 0s - loss: 8.9591 - acc: 0.373 - ETA: 0s - loss: 8.9332 - acc: 0.375 - ETA: 0s - loss: 8.9314 - acc: 0.375 - ETA: 0s - loss: 8.9380 - acc: 0.375 - ETA: 0s - loss: 8.9268 - acc: 0.375 - ETA: 0s - loss: 8.9136 - acc: 0.376 - ETA: 0s - loss: 8.9303 - acc: 0.375 - 3s 479us/step - loss: 8.9384 - acc: 0.3746 - val_loss: 9.2045 - val_acc: 0.3305\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.44205 to 9.20452, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 9.0913 - acc: 0.400 - ETA: 2s - loss: 8.8717 - acc: 0.400 - ETA: 2s - loss: 7.8370 - acc: 0.457 - ETA: 2s - loss: 8.1142 - acc: 0.447 - ETA: 2s - loss: 8.0932 - acc: 0.452 - ETA: 2s - loss: 8.0550 - acc: 0.445 - ETA: 2s - loss: 8.4529 - acc: 0.420 - ETA: 2s - loss: 8.5318 - acc: 0.414 - ETA: 2s - loss: 8.3820 - acc: 0.424 - ETA: 2s - loss: 8.3500 - acc: 0.425 - ETA: 2s - loss: 8.2820 - acc: 0.430 - ETA: 2s - loss: 8.4464 - acc: 0.420 - ETA: 2s - loss: 8.5329 - acc: 0.412 - ETA: 2s - loss: 8.4651 - acc: 0.417 - ETA: 2s - loss: 8.4328 - acc: 0.417 - ETA: 2s - loss: 8.3488 - acc: 0.421 - ETA: 2s - loss: 8.3985 - acc: 0.419 - ETA: 1s - loss: 8.4104 - acc: 0.417 - ETA: 1s - loss: 8.4088 - acc: 0.417 - ETA: 1s - loss: 8.4231 - acc: 0.416 - ETA: 1s - loss: 8.4692 - acc: 0.414 - ETA: 1s - loss: 8.4855 - acc: 0.414 - ETA: 1s - loss: 8.5445 - acc: 0.412 - ETA: 1s - loss: 8.5068 - acc: 0.415 - ETA: 1s - loss: 8.5379 - acc: 0.413 - ETA: 1s - loss: 8.5094 - acc: 0.415 - ETA: 1s - loss: 8.5297 - acc: 0.414 - ETA: 1s - loss: 8.4847 - acc: 0.415 - ETA: 1s - loss: 8.5604 - acc: 0.411 - ETA: 1s - loss: 8.5597 - acc: 0.411 - ETA: 1s - loss: 8.5659 - acc: 0.410 - ETA: 1s - loss: 8.5510 - acc: 0.411 - ETA: 1s - loss: 8.5306 - acc: 0.412 - ETA: 1s - loss: 8.5374 - acc: 0.412 - ETA: 1s - loss: 8.5457 - acc: 0.412 - ETA: 1s - loss: 8.5569 - acc: 0.411 - ETA: 0s - loss: 8.5287 - acc: 0.413 - ETA: 0s - loss: 8.5356 - acc: 0.412 - ETA: 0s - loss: 8.5526 - acc: 0.411 - ETA: 0s - loss: 8.5571 - acc: 0.411 - ETA: 0s - loss: 8.5128 - acc: 0.414 - ETA: 0s - loss: 8.5168 - acc: 0.413 - ETA: 0s - loss: 8.5391 - acc: 0.412 - ETA: 0s - loss: 8.4923 - acc: 0.414 - ETA: 0s - loss: 8.5126 - acc: 0.413 - ETA: 0s - loss: 8.5038 - acc: 0.413 - ETA: 0s - loss: 8.5130 - acc: 0.413 - ETA: 0s - loss: 8.5336 - acc: 0.411 - ETA: 0s - loss: 8.5371 - acc: 0.411 - ETA: 0s - loss: 8.5266 - acc: 0.412 - ETA: 0s - loss: 8.5062 - acc: 0.412 - ETA: 0s - loss: 8.4996 - acc: 0.413 - ETA: 0s - loss: 8.5022 - acc: 0.413 - ETA: 0s - loss: 8.4938 - acc: 0.413 - 3s 448us/step - loss: 8.4900 - acc: 0.4144 - val_loss: 8.9084 - val_acc: 0.3437\n",
      "\n",
      "Epoch 00004: val_loss improved from 9.20452 to 8.90843, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 7.0410 - acc: 0.450 - ETA: 2s - loss: 8.6990 - acc: 0.392 - ETA: 2s - loss: 7.9769 - acc: 0.442 - ETA: 2s - loss: 8.0810 - acc: 0.432 - ETA: 2s - loss: 7.9345 - acc: 0.450 - ETA: 2s - loss: 7.9102 - acc: 0.456 - ETA: 2s - loss: 8.0296 - acc: 0.451 - ETA: 2s - loss: 7.9718 - acc: 0.454 - ETA: 2s - loss: 8.0387 - acc: 0.452 - ETA: 2s - loss: 8.0366 - acc: 0.447 - ETA: 2s - loss: 8.0362 - acc: 0.447 - ETA: 2s - loss: 8.1141 - acc: 0.443 - ETA: 2s - loss: 8.2154 - acc: 0.438 - ETA: 2s - loss: 8.1391 - acc: 0.444 - ETA: 2s - loss: 8.1512 - acc: 0.445 - ETA: 1s - loss: 8.0918 - acc: 0.450 - ETA: 1s - loss: 8.1467 - acc: 0.448 - ETA: 1s - loss: 8.2304 - acc: 0.442 - ETA: 1s - loss: 8.2352 - acc: 0.440 - ETA: 1s - loss: 8.2157 - acc: 0.441 - ETA: 1s - loss: 8.2343 - acc: 0.438 - ETA: 1s - loss: 8.2269 - acc: 0.440 - ETA: 1s - loss: 8.2330 - acc: 0.438 - ETA: 1s - loss: 8.2797 - acc: 0.435 - ETA: 1s - loss: 8.2806 - acc: 0.435 - ETA: 1s - loss: 8.3021 - acc: 0.434 - ETA: 1s - loss: 8.3142 - acc: 0.433 - ETA: 1s - loss: 8.2995 - acc: 0.433 - ETA: 1s - loss: 8.3069 - acc: 0.433 - ETA: 1s - loss: 8.2740 - acc: 0.435 - ETA: 1s - loss: 8.2599 - acc: 0.436 - ETA: 1s - loss: 8.2076 - acc: 0.440 - ETA: 1s - loss: 8.2363 - acc: 0.438 - ETA: 1s - loss: 8.2468 - acc: 0.438 - ETA: 0s - loss: 8.2177 - acc: 0.439 - ETA: 0s - loss: 8.2028 - acc: 0.441 - ETA: 0s - loss: 8.2107 - acc: 0.440 - ETA: 0s - loss: 8.1972 - acc: 0.441 - ETA: 0s - loss: 8.1935 - acc: 0.441 - ETA: 0s - loss: 8.1846 - acc: 0.441 - ETA: 0s - loss: 8.1853 - acc: 0.441 - ETA: 0s - loss: 8.1747 - acc: 0.441 - ETA: 0s - loss: 8.1810 - acc: 0.441 - ETA: 0s - loss: 8.1508 - acc: 0.442 - ETA: 0s - loss: 8.1607 - acc: 0.441 - ETA: 0s - loss: 8.1585 - acc: 0.441 - ETA: 0s - loss: 8.1440 - acc: 0.442 - ETA: 0s - loss: 8.1420 - acc: 0.443 - ETA: 0s - loss: 8.1253 - acc: 0.443 - ETA: 0s - loss: 8.1079 - acc: 0.444 - ETA: 0s - loss: 8.1132 - acc: 0.444 - ETA: 0s - loss: 8.1209 - acc: 0.444 - ETA: 0s - loss: 8.0963 - acc: 0.445 - 3s 446us/step - loss: 8.1031 - acc: 0.4455 - val_loss: 8.6430 - val_acc: 0.3653\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.90843 to 8.64298, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 8.1284 - acc: 0.400 - ETA: 2s - loss: 7.4980 - acc: 0.485 - ETA: 2s - loss: 7.3518 - acc: 0.507 - ETA: 2s - loss: 7.3058 - acc: 0.507 - ETA: 2s - loss: 7.5722 - acc: 0.486 - ETA: 2s - loss: 7.5729 - acc: 0.484 - ETA: 2s - loss: 7.6192 - acc: 0.480 - ETA: 2s - loss: 7.6381 - acc: 0.477 - ETA: 2s - loss: 7.7180 - acc: 0.471 - ETA: 2s - loss: 7.7282 - acc: 0.470 - ETA: 2s - loss: 7.7534 - acc: 0.471 - ETA: 2s - loss: 7.7640 - acc: 0.472 - ETA: 2s - loss: 7.7518 - acc: 0.473 - ETA: 2s - loss: 7.8686 - acc: 0.467 - ETA: 2s - loss: 7.8342 - acc: 0.468 - ETA: 2s - loss: 7.8287 - acc: 0.469 - ETA: 2s - loss: 7.8716 - acc: 0.466 - ETA: 2s - loss: 7.8456 - acc: 0.468 - ETA: 1s - loss: 7.8752 - acc: 0.466 - ETA: 1s - loss: 7.8243 - acc: 0.469 - ETA: 1s - loss: 7.8040 - acc: 0.471 - ETA: 1s - loss: 7.7605 - acc: 0.474 - ETA: 1s - loss: 7.7997 - acc: 0.471 - ETA: 1s - loss: 7.7574 - acc: 0.475 - ETA: 1s - loss: 7.7863 - acc: 0.473 - ETA: 1s - loss: 7.8041 - acc: 0.470 - ETA: 1s - loss: 7.7634 - acc: 0.473 - ETA: 1s - loss: 7.7961 - acc: 0.471 - ETA: 1s - loss: 7.8089 - acc: 0.470 - ETA: 1s - loss: 7.8127 - acc: 0.469 - ETA: 1s - loss: 7.7930 - acc: 0.470 - ETA: 1s - loss: 7.7954 - acc: 0.470 - ETA: 1s - loss: 7.8376 - acc: 0.468 - ETA: 1s - loss: 7.8451 - acc: 0.468 - ETA: 1s - loss: 7.8361 - acc: 0.468 - ETA: 1s - loss: 7.8346 - acc: 0.467 - ETA: 1s - loss: 7.8395 - acc: 0.466 - ETA: 0s - loss: 7.8277 - acc: 0.466 - ETA: 0s - loss: 7.8168 - acc: 0.466 - ETA: 0s - loss: 7.7809 - acc: 0.469 - ETA: 0s - loss: 7.7838 - acc: 0.468 - ETA: 0s - loss: 7.7761 - acc: 0.469 - ETA: 0s - loss: 7.7998 - acc: 0.467 - ETA: 0s - loss: 7.8026 - acc: 0.467 - ETA: 0s - loss: 7.7942 - acc: 0.468 - ETA: 0s - loss: 7.7891 - acc: 0.468 - ETA: 0s - loss: 7.7778 - acc: 0.468 - ETA: 0s - loss: 7.7885 - acc: 0.468 - ETA: 0s - loss: 7.7962 - acc: 0.468 - ETA: 0s - loss: 7.7695 - acc: 0.469 - ETA: 0s - loss: 7.7493 - acc: 0.469 - ETA: 0s - loss: 7.7205 - acc: 0.471 - ETA: 0s - loss: 7.7105 - acc: 0.471 - ETA: 0s - loss: 7.7190 - acc: 0.471 - ETA: 0s - loss: 7.7292 - acc: 0.470 - ETA: 0s - loss: 7.7459 - acc: 0.469 - 3s 467us/step - loss: 7.7454 - acc: 0.4696 - val_loss: 8.2461 - val_acc: 0.3892\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.64298 to 8.24612, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 6.4475 - acc: 0.600 - ETA: 2s - loss: 7.0306 - acc: 0.514 - ETA: 2s - loss: 7.1700 - acc: 0.519 - ETA: 2s - loss: 7.4682 - acc: 0.497 - ETA: 2s - loss: 7.6069 - acc: 0.486 - ETA: 2s - loss: 7.5277 - acc: 0.491 - ETA: 2s - loss: 7.5605 - acc: 0.490 - ETA: 2s - loss: 7.5263 - acc: 0.495 - ETA: 2s - loss: 7.6303 - acc: 0.487 - ETA: 2s - loss: 7.6621 - acc: 0.487 - ETA: 2s - loss: 7.6082 - acc: 0.489 - ETA: 2s - loss: 7.5550 - acc: 0.491 - ETA: 2s - loss: 7.4967 - acc: 0.495 - ETA: 2s - loss: 7.4747 - acc: 0.496 - ETA: 2s - loss: 7.5193 - acc: 0.495 - ETA: 2s - loss: 7.5424 - acc: 0.494 - ETA: 2s - loss: 7.5024 - acc: 0.496 - ETA: 2s - loss: 7.5110 - acc: 0.496 - ETA: 1s - loss: 7.5431 - acc: 0.493 - ETA: 1s - loss: 7.5519 - acc: 0.493 - ETA: 1s - loss: 7.6210 - acc: 0.487 - ETA: 1s - loss: 7.5401 - acc: 0.493 - ETA: 1s - loss: 7.5650 - acc: 0.492 - ETA: 1s - loss: 7.5631 - acc: 0.491 - ETA: 1s - loss: 7.5571 - acc: 0.491 - ETA: 1s - loss: 7.5853 - acc: 0.489 - ETA: 1s - loss: 7.5841 - acc: 0.488 - ETA: 1s - loss: 7.5890 - acc: 0.488 - ETA: 1s - loss: 7.6037 - acc: 0.487 - ETA: 1s - loss: 7.6087 - acc: 0.486 - ETA: 1s - loss: 7.5955 - acc: 0.487 - ETA: 1s - loss: 7.5801 - acc: 0.488 - ETA: 1s - loss: 7.5608 - acc: 0.489 - ETA: 1s - loss: 7.5542 - acc: 0.488 - ETA: 1s - loss: 7.5594 - acc: 0.488 - ETA: 1s - loss: 7.5841 - acc: 0.486 - ETA: 1s - loss: 7.5995 - acc: 0.485 - ETA: 0s - loss: 7.5880 - acc: 0.485 - ETA: 0s - loss: 7.5928 - acc: 0.484 - ETA: 0s - loss: 7.5668 - acc: 0.486 - ETA: 0s - loss: 7.5708 - acc: 0.486 - ETA: 0s - loss: 7.5826 - acc: 0.485 - ETA: 0s - loss: 7.5723 - acc: 0.486 - ETA: 0s - loss: 7.5224 - acc: 0.489 - ETA: 0s - loss: 7.5243 - acc: 0.489 - ETA: 0s - loss: 7.5062 - acc: 0.490 - ETA: 0s - loss: 7.5007 - acc: 0.491 - ETA: 0s - loss: 7.5028 - acc: 0.492 - ETA: 0s - loss: 7.5013 - acc: 0.492 - ETA: 0s - loss: 7.4970 - acc: 0.492 - ETA: 0s - loss: 7.4842 - acc: 0.492 - ETA: 0s - loss: 7.4651 - acc: 0.493 - ETA: 0s - loss: 7.4746 - acc: 0.493 - ETA: 0s - loss: 7.4767 - acc: 0.492 - ETA: 0s - loss: 7.4883 - acc: 0.491 - 3s 455us/step - loss: 7.4691 - acc: 0.4922 - val_loss: 7.9589 - val_acc: 0.4084\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.24612 to 7.95887, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 4.9372 - acc: 0.650 - ETA: 2s - loss: 6.2319 - acc: 0.568 - ETA: 2s - loss: 7.1565 - acc: 0.503 - ETA: 2s - loss: 7.1590 - acc: 0.509 - ETA: 2s - loss: 6.8967 - acc: 0.530 - ETA: 2s - loss: 7.0554 - acc: 0.522 - ETA: 2s - loss: 7.0935 - acc: 0.522 - ETA: 2s - loss: 7.2246 - acc: 0.516 - ETA: 2s - loss: 7.1986 - acc: 0.513 - ETA: 2s - loss: 7.1700 - acc: 0.516 - ETA: 2s - loss: 7.1575 - acc: 0.516 - ETA: 2s - loss: 7.1537 - acc: 0.516 - ETA: 2s - loss: 7.1655 - acc: 0.515 - ETA: 2s - loss: 7.2237 - acc: 0.509 - ETA: 1s - loss: 7.3038 - acc: 0.505 - ETA: 1s - loss: 7.2680 - acc: 0.508 - ETA: 1s - loss: 7.3386 - acc: 0.505 - ETA: 1s - loss: 7.2775 - acc: 0.508 - ETA: 1s - loss: 7.3023 - acc: 0.507 - ETA: 1s - loss: 7.2941 - acc: 0.507 - ETA: 1s - loss: 7.2355 - acc: 0.511 - ETA: 1s - loss: 7.2685 - acc: 0.509 - ETA: 1s - loss: 7.2023 - acc: 0.512 - ETA: 1s - loss: 7.2511 - acc: 0.509 - ETA: 1s - loss: 7.2391 - acc: 0.511 - ETA: 1s - loss: 7.2465 - acc: 0.511 - ETA: 1s - loss: 7.2227 - acc: 0.512 - ETA: 1s - loss: 7.2241 - acc: 0.512 - ETA: 1s - loss: 7.2617 - acc: 0.511 - ETA: 1s - loss: 7.2320 - acc: 0.513 - ETA: 1s - loss: 7.2361 - acc: 0.513 - ETA: 1s - loss: 7.2171 - acc: 0.514 - ETA: 0s - loss: 7.2400 - acc: 0.513 - ETA: 0s - loss: 7.2500 - acc: 0.513 - ETA: 0s - loss: 7.2412 - acc: 0.514 - ETA: 0s - loss: 7.2190 - acc: 0.515 - ETA: 0s - loss: 7.2061 - acc: 0.516 - ETA: 0s - loss: 7.1721 - acc: 0.517 - ETA: 0s - loss: 7.1640 - acc: 0.517 - ETA: 0s - loss: 7.1728 - acc: 0.516 - ETA: 0s - loss: 7.1656 - acc: 0.516 - ETA: 0s - loss: 7.1638 - acc: 0.517 - ETA: 0s - loss: 7.1564 - acc: 0.518 - ETA: 0s - loss: 7.1400 - acc: 0.518 - ETA: 0s - loss: 7.1265 - acc: 0.519 - ETA: 0s - loss: 7.1266 - acc: 0.519 - ETA: 0s - loss: 7.1116 - acc: 0.519 - ETA: 0s - loss: 7.1400 - acc: 0.517 - ETA: 0s - loss: 7.1425 - acc: 0.517 - ETA: 0s - loss: 7.1517 - acc: 0.515 - ETA: 0s - loss: 7.1365 - acc: 0.517 - ETA: 0s - loss: 7.1367 - acc: 0.516 - 3s 438us/step - loss: 7.1397 - acc: 0.5165 - val_loss: 7.8317 - val_acc: 0.4311\n",
      "\n",
      "Epoch 00008: val_loss improved from 7.95887 to 7.83169, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 6.5532 - acc: 0.550 - ETA: 2s - loss: 6.2918 - acc: 0.528 - ETA: 2s - loss: 6.3418 - acc: 0.553 - ETA: 2s - loss: 6.9495 - acc: 0.523 - ETA: 2s - loss: 7.3724 - acc: 0.496 - ETA: 2s - loss: 7.3571 - acc: 0.498 - ETA: 2s - loss: 7.2626 - acc: 0.508 - ETA: 2s - loss: 7.1745 - acc: 0.513 - ETA: 2s - loss: 7.1017 - acc: 0.517 - ETA: 2s - loss: 7.0841 - acc: 0.519 - ETA: 2s - loss: 7.2259 - acc: 0.512 - ETA: 2s - loss: 7.1933 - acc: 0.514 - ETA: 2s - loss: 7.1503 - acc: 0.515 - ETA: 2s - loss: 7.1424 - acc: 0.515 - ETA: 2s - loss: 7.1112 - acc: 0.520 - ETA: 2s - loss: 7.0760 - acc: 0.522 - ETA: 1s - loss: 7.0692 - acc: 0.522 - ETA: 1s - loss: 7.0034 - acc: 0.527 - ETA: 1s - loss: 7.0502 - acc: 0.525 - ETA: 1s - loss: 7.0396 - acc: 0.526 - ETA: 1s - loss: 7.0082 - acc: 0.529 - ETA: 1s - loss: 7.0747 - acc: 0.526 - ETA: 1s - loss: 7.0626 - acc: 0.527 - ETA: 1s - loss: 7.0422 - acc: 0.527 - ETA: 1s - loss: 7.0377 - acc: 0.527 - ETA: 1s - loss: 7.0303 - acc: 0.527 - ETA: 1s - loss: 6.9848 - acc: 0.530 - ETA: 1s - loss: 6.9532 - acc: 0.532 - ETA: 1s - loss: 6.9560 - acc: 0.533 - ETA: 1s - loss: 6.9548 - acc: 0.533 - ETA: 1s - loss: 6.9985 - acc: 0.530 - ETA: 1s - loss: 6.9885 - acc: 0.531 - ETA: 1s - loss: 6.9618 - acc: 0.533 - ETA: 1s - loss: 6.9515 - acc: 0.534 - ETA: 1s - loss: 6.9161 - acc: 0.536 - ETA: 0s - loss: 6.9121 - acc: 0.536 - ETA: 0s - loss: 6.8711 - acc: 0.538 - ETA: 0s - loss: 6.8617 - acc: 0.539 - ETA: 0s - loss: 6.8664 - acc: 0.538 - ETA: 0s - loss: 6.8576 - acc: 0.539 - ETA: 0s - loss: 6.8586 - acc: 0.539 - ETA: 0s - loss: 6.8815 - acc: 0.538 - ETA: 0s - loss: 6.9096 - acc: 0.535 - ETA: 0s - loss: 6.8757 - acc: 0.537 - ETA: 0s - loss: 6.8768 - acc: 0.538 - ETA: 0s - loss: 6.8797 - acc: 0.537 - ETA: 0s - loss: 6.9046 - acc: 0.535 - ETA: 0s - loss: 6.8903 - acc: 0.536 - ETA: 0s - loss: 6.8643 - acc: 0.537 - ETA: 0s - loss: 6.8677 - acc: 0.537 - ETA: 0s - loss: 6.8640 - acc: 0.536 - ETA: 0s - loss: 6.8554 - acc: 0.536 - ETA: 0s - loss: 6.8455 - acc: 0.537 - 3s 441us/step - loss: 6.8396 - acc: 0.5374 - val_loss: 7.3504 - val_acc: 0.4371\n",
      "\n",
      "Epoch 00009: val_loss improved from 7.83169 to 7.35039, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 6.6064 - acc: 0.550 - ETA: 2s - loss: 6.1610 - acc: 0.578 - ETA: 2s - loss: 6.3152 - acc: 0.582 - ETA: 2s - loss: 6.3778 - acc: 0.577 - ETA: 2s - loss: 6.3794 - acc: 0.578 - ETA: 2s - loss: 6.3171 - acc: 0.584 - ETA: 2s - loss: 6.3546 - acc: 0.577 - ETA: 2s - loss: 6.4496 - acc: 0.573 - ETA: 2s - loss: 6.5529 - acc: 0.564 - ETA: 2s - loss: 6.4725 - acc: 0.568 - ETA: 2s - loss: 6.4780 - acc: 0.568 - ETA: 2s - loss: 6.5179 - acc: 0.566 - ETA: 2s - loss: 6.4803 - acc: 0.568 - ETA: 2s - loss: 6.3953 - acc: 0.574 - ETA: 2s - loss: 6.4499 - acc: 0.569 - ETA: 2s - loss: 6.4220 - acc: 0.571 - ETA: 1s - loss: 6.4468 - acc: 0.570 - ETA: 1s - loss: 6.4734 - acc: 0.569 - ETA: 1s - loss: 6.4438 - acc: 0.570 - ETA: 1s - loss: 6.4693 - acc: 0.568 - ETA: 1s - loss: 6.4178 - acc: 0.571 - ETA: 1s - loss: 6.3929 - acc: 0.572 - ETA: 1s - loss: 6.4424 - acc: 0.568 - ETA: 1s - loss: 6.4737 - acc: 0.565 - ETA: 1s - loss: 6.4847 - acc: 0.565 - ETA: 1s - loss: 6.4810 - acc: 0.564 - ETA: 1s - loss: 6.4597 - acc: 0.565 - ETA: 1s - loss: 6.4813 - acc: 0.565 - ETA: 1s - loss: 6.4738 - acc: 0.566 - ETA: 1s - loss: 6.4996 - acc: 0.563 - ETA: 1s - loss: 6.5366 - acc: 0.561 - ETA: 1s - loss: 6.5590 - acc: 0.559 - ETA: 1s - loss: 6.5590 - acc: 0.558 - ETA: 1s - loss: 6.5308 - acc: 0.559 - ETA: 1s - loss: 6.4789 - acc: 0.562 - ETA: 1s - loss: 6.4785 - acc: 0.562 - ETA: 0s - loss: 6.4487 - acc: 0.563 - ETA: 0s - loss: 6.4821 - acc: 0.561 - ETA: 0s - loss: 6.4913 - acc: 0.561 - ETA: 0s - loss: 6.5082 - acc: 0.560 - ETA: 0s - loss: 6.4899 - acc: 0.561 - ETA: 0s - loss: 6.4889 - acc: 0.561 - ETA: 0s - loss: 6.4866 - acc: 0.561 - ETA: 0s - loss: 6.4709 - acc: 0.562 - ETA: 0s - loss: 6.4637 - acc: 0.562 - ETA: 0s - loss: 6.4585 - acc: 0.562 - ETA: 0s - loss: 6.4566 - acc: 0.562 - ETA: 0s - loss: 6.4499 - acc: 0.562 - ETA: 0s - loss: 6.4445 - acc: 0.563 - ETA: 0s - loss: 6.4315 - acc: 0.564 - ETA: 0s - loss: 6.4517 - acc: 0.562 - ETA: 0s - loss: 6.4451 - acc: 0.563 - ETA: 0s - loss: 6.4482 - acc: 0.562 - ETA: 0s - loss: 6.4458 - acc: 0.563 - ETA: 0s - loss: 6.4555 - acc: 0.562 - 3s 446us/step - loss: 6.4584 - acc: 0.5620 - val_loss: 7.0560 - val_acc: 0.4551\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.35039 to 7.05596, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.8419 - acc: 0.700 - ETA: 2s - loss: 6.5945 - acc: 0.564 - ETA: 2s - loss: 6.7025 - acc: 0.553 - ETA: 2s - loss: 6.5398 - acc: 0.567 - ETA: 2s - loss: 6.2704 - acc: 0.581 - ETA: 2s - loss: 6.2999 - acc: 0.577 - ETA: 2s - loss: 6.2774 - acc: 0.578 - ETA: 2s - loss: 6.2862 - acc: 0.578 - ETA: 2s - loss: 6.2193 - acc: 0.581 - ETA: 2s - loss: 6.2360 - acc: 0.579 - ETA: 2s - loss: 6.2247 - acc: 0.580 - ETA: 2s - loss: 6.2848 - acc: 0.576 - ETA: 2s - loss: 6.1924 - acc: 0.582 - ETA: 2s - loss: 6.1647 - acc: 0.585 - ETA: 2s - loss: 6.2458 - acc: 0.581 - ETA: 1s - loss: 6.3163 - acc: 0.576 - ETA: 1s - loss: 6.2748 - acc: 0.578 - ETA: 1s - loss: 6.1355 - acc: 0.585 - ETA: 1s - loss: 6.1571 - acc: 0.585 - ETA: 1s - loss: 6.1823 - acc: 0.582 - ETA: 1s - loss: 6.1631 - acc: 0.583 - ETA: 1s - loss: 6.1263 - acc: 0.583 - ETA: 1s - loss: 6.1374 - acc: 0.582 - ETA: 1s - loss: 6.1340 - acc: 0.581 - ETA: 1s - loss: 6.1720 - acc: 0.579 - ETA: 1s - loss: 6.1773 - acc: 0.580 - ETA: 1s - loss: 6.1735 - acc: 0.579 - ETA: 1s - loss: 6.1653 - acc: 0.580 - ETA: 1s - loss: 6.1419 - acc: 0.581 - ETA: 1s - loss: 6.1431 - acc: 0.581 - ETA: 1s - loss: 6.1771 - acc: 0.578 - ETA: 1s - loss: 6.1612 - acc: 0.580 - ETA: 1s - loss: 6.1317 - acc: 0.581 - ETA: 1s - loss: 6.1316 - acc: 0.581 - ETA: 0s - loss: 6.1048 - acc: 0.583 - ETA: 0s - loss: 6.1019 - acc: 0.583 - ETA: 0s - loss: 6.0861 - acc: 0.583 - ETA: 0s - loss: 6.0804 - acc: 0.584 - ETA: 0s - loss: 6.0756 - acc: 0.583 - ETA: 0s - loss: 6.0586 - acc: 0.584 - ETA: 0s - loss: 6.0288 - acc: 0.586 - ETA: 0s - loss: 6.0582 - acc: 0.584 - ETA: 0s - loss: 6.0453 - acc: 0.584 - ETA: 0s - loss: 6.0468 - acc: 0.584 - ETA: 0s - loss: 6.0609 - acc: 0.584 - ETA: 0s - loss: 6.0676 - acc: 0.583 - ETA: 0s - loss: 6.0636 - acc: 0.583 - ETA: 0s - loss: 6.0538 - acc: 0.584 - ETA: 0s - loss: 6.0214 - acc: 0.585 - ETA: 0s - loss: 6.0203 - acc: 0.585 - ETA: 0s - loss: 5.9998 - acc: 0.586 - ETA: 0s - loss: 6.0196 - acc: 0.585 - ETA: 0s - loss: 6.0418 - acc: 0.584 - 3s 443us/step - loss: 6.0527 - acc: 0.5834 - val_loss: 6.8112 - val_acc: 0.4814\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.05596 to 6.81122, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 3.4544 - acc: 0.700 - ETA: 2s - loss: 5.0400 - acc: 0.650 - ETA: 2s - loss: 5.1119 - acc: 0.653 - ETA: 2s - loss: 5.5639 - acc: 0.625 - ETA: 2s - loss: 5.4549 - acc: 0.632 - ETA: 2s - loss: 5.4840 - acc: 0.631 - ETA: 2s - loss: 5.4747 - acc: 0.633 - ETA: 2s - loss: 5.6193 - acc: 0.623 - ETA: 2s - loss: 5.6639 - acc: 0.618 - ETA: 2s - loss: 5.6498 - acc: 0.621 - ETA: 2s - loss: 5.7351 - acc: 0.615 - ETA: 2s - loss: 5.8346 - acc: 0.609 - ETA: 2s - loss: 5.7919 - acc: 0.611 - ETA: 2s - loss: 5.7466 - acc: 0.613 - ETA: 2s - loss: 5.8316 - acc: 0.610 - ETA: 1s - loss: 5.8257 - acc: 0.610 - ETA: 1s - loss: 5.8187 - acc: 0.612 - ETA: 1s - loss: 5.8199 - acc: 0.612 - ETA: 1s - loss: 5.7985 - acc: 0.613 - ETA: 1s - loss: 5.8183 - acc: 0.610 - ETA: 1s - loss: 5.8817 - acc: 0.605 - ETA: 1s - loss: 5.8791 - acc: 0.604 - ETA: 1s - loss: 5.8881 - acc: 0.603 - ETA: 1s - loss: 5.8689 - acc: 0.605 - ETA: 1s - loss: 5.8777 - acc: 0.605 - ETA: 1s - loss: 5.8745 - acc: 0.605 - ETA: 1s - loss: 5.8742 - acc: 0.606 - ETA: 1s - loss: 5.9166 - acc: 0.603 - ETA: 1s - loss: 5.9102 - acc: 0.603 - ETA: 1s - loss: 5.8959 - acc: 0.603 - ETA: 1s - loss: 5.8844 - acc: 0.605 - ETA: 1s - loss: 5.8874 - acc: 0.604 - ETA: 1s - loss: 5.8496 - acc: 0.606 - ETA: 1s - loss: 5.8722 - acc: 0.606 - ETA: 0s - loss: 5.8744 - acc: 0.605 - ETA: 0s - loss: 5.8827 - acc: 0.604 - ETA: 0s - loss: 5.8822 - acc: 0.603 - ETA: 0s - loss: 5.8942 - acc: 0.603 - ETA: 0s - loss: 5.8820 - acc: 0.605 - ETA: 0s - loss: 5.8931 - acc: 0.604 - ETA: 0s - loss: 5.8553 - acc: 0.606 - ETA: 0s - loss: 5.8319 - acc: 0.607 - ETA: 0s - loss: 5.8405 - acc: 0.606 - ETA: 0s - loss: 5.8516 - acc: 0.606 - ETA: 0s - loss: 5.8459 - acc: 0.605 - ETA: 0s - loss: 5.8434 - acc: 0.606 - ETA: 0s - loss: 5.8380 - acc: 0.606 - ETA: 0s - loss: 5.8246 - acc: 0.607 - ETA: 0s - loss: 5.8225 - acc: 0.606 - ETA: 0s - loss: 5.8174 - acc: 0.607 - ETA: 0s - loss: 5.7873 - acc: 0.609 - ETA: 0s - loss: 5.7838 - acc: 0.610 - ETA: 0s - loss: 5.7871 - acc: 0.610 - 3s 443us/step - loss: 5.7763 - acc: 0.6112 - val_loss: 6.6090 - val_acc: 0.4802\n",
      "\n",
      "Epoch 00012: val_loss improved from 6.81122 to 6.60897, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 5.6103 - acc: 0.650 - ETA: 2s - loss: 5.0218 - acc: 0.662 - ETA: 2s - loss: 5.0829 - acc: 0.656 - ETA: 2s - loss: 5.1800 - acc: 0.647 - ETA: 2s - loss: 5.5789 - acc: 0.625 - ETA: 2s - loss: 5.7571 - acc: 0.618 - ETA: 2s - loss: 5.8245 - acc: 0.617 - ETA: 2s - loss: 5.7990 - acc: 0.618 - ETA: 2s - loss: 5.8872 - acc: 0.613 - ETA: 2s - loss: 5.8413 - acc: 0.617 - ETA: 2s - loss: 5.7639 - acc: 0.623 - ETA: 2s - loss: 5.6986 - acc: 0.625 - ETA: 2s - loss: 5.6761 - acc: 0.625 - ETA: 2s - loss: 5.6607 - acc: 0.626 - ETA: 1s - loss: 5.7139 - acc: 0.623 - ETA: 1s - loss: 5.6606 - acc: 0.627 - ETA: 1s - loss: 5.6087 - acc: 0.629 - ETA: 1s - loss: 5.6152 - acc: 0.629 - ETA: 1s - loss: 5.5531 - acc: 0.634 - ETA: 1s - loss: 5.5980 - acc: 0.631 - ETA: 1s - loss: 5.5727 - acc: 0.633 - ETA: 1s - loss: 5.6143 - acc: 0.630 - ETA: 1s - loss: 5.6406 - acc: 0.629 - ETA: 1s - loss: 5.6438 - acc: 0.628 - ETA: 1s - loss: 5.6343 - acc: 0.629 - ETA: 1s - loss: 5.6341 - acc: 0.628 - ETA: 1s - loss: 5.6512 - acc: 0.627 - ETA: 1s - loss: 5.6154 - acc: 0.630 - ETA: 1s - loss: 5.6007 - acc: 0.630 - ETA: 1s - loss: 5.5763 - acc: 0.631 - ETA: 1s - loss: 5.6292 - acc: 0.628 - ETA: 1s - loss: 5.6202 - acc: 0.627 - ETA: 1s - loss: 5.6377 - acc: 0.626 - ETA: 1s - loss: 5.6117 - acc: 0.628 - ETA: 0s - loss: 5.6031 - acc: 0.629 - ETA: 0s - loss: 5.6362 - acc: 0.627 - ETA: 0s - loss: 5.6501 - acc: 0.626 - ETA: 0s - loss: 5.6712 - acc: 0.624 - ETA: 0s - loss: 5.6821 - acc: 0.624 - ETA: 0s - loss: 5.6638 - acc: 0.624 - ETA: 0s - loss: 5.6373 - acc: 0.625 - ETA: 0s - loss: 5.6472 - acc: 0.624 - ETA: 0s - loss: 5.6226 - acc: 0.626 - ETA: 0s - loss: 5.6332 - acc: 0.625 - ETA: 0s - loss: 5.6227 - acc: 0.625 - ETA: 0s - loss: 5.5948 - acc: 0.625 - ETA: 0s - loss: 5.5908 - acc: 0.626 - ETA: 0s - loss: 5.5971 - acc: 0.626 - ETA: 0s - loss: 5.6119 - acc: 0.624 - ETA: 0s - loss: 5.6109 - acc: 0.625 - ETA: 0s - loss: 5.6278 - acc: 0.624 - ETA: 0s - loss: 5.6229 - acc: 0.624 - ETA: 0s - loss: 5.6125 - acc: 0.624 - 3s 441us/step - loss: 5.6253 - acc: 0.6238 - val_loss: 6.5959 - val_acc: 0.4778\n",
      "\n",
      "Epoch 00013: val_loss improved from 6.60897 to 6.59588, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 5.7551 - acc: 0.600 - ETA: 2s - loss: 5.3951 - acc: 0.635 - ETA: 2s - loss: 5.8735 - acc: 0.607 - ETA: 2s - loss: 5.5662 - acc: 0.628 - ETA: 2s - loss: 5.5415 - acc: 0.632 - ETA: 2s - loss: 5.5386 - acc: 0.634 - ETA: 2s - loss: 5.4914 - acc: 0.636 - ETA: 2s - loss: 5.4501 - acc: 0.638 - ETA: 2s - loss: 5.4404 - acc: 0.636 - ETA: 2s - loss: 5.3962 - acc: 0.640 - ETA: 2s - loss: 5.2589 - acc: 0.650 - ETA: 2s - loss: 5.2792 - acc: 0.649 - ETA: 2s - loss: 5.3447 - acc: 0.645 - ETA: 2s - loss: 5.4002 - acc: 0.642 - ETA: 2s - loss: 5.3294 - acc: 0.647 - ETA: 2s - loss: 5.3325 - acc: 0.647 - ETA: 1s - loss: 5.2783 - acc: 0.650 - ETA: 1s - loss: 5.3499 - acc: 0.645 - ETA: 1s - loss: 5.3671 - acc: 0.644 - ETA: 1s - loss: 5.3825 - acc: 0.644 - ETA: 1s - loss: 5.4224 - acc: 0.641 - ETA: 1s - loss: 5.4290 - acc: 0.640 - ETA: 1s - loss: 5.4169 - acc: 0.641 - ETA: 1s - loss: 5.3879 - acc: 0.644 - ETA: 1s - loss: 5.3629 - acc: 0.645 - ETA: 1s - loss: 5.3269 - acc: 0.647 - ETA: 1s - loss: 5.3204 - acc: 0.648 - ETA: 1s - loss: 5.3056 - acc: 0.650 - ETA: 1s - loss: 5.3223 - acc: 0.649 - ETA: 1s - loss: 5.2692 - acc: 0.653 - ETA: 1s - loss: 5.2848 - acc: 0.652 - ETA: 1s - loss: 5.2856 - acc: 0.652 - ETA: 1s - loss: 5.3131 - acc: 0.651 - ETA: 1s - loss: 5.2982 - acc: 0.651 - ETA: 1s - loss: 5.3277 - acc: 0.649 - ETA: 0s - loss: 5.3383 - acc: 0.649 - ETA: 0s - loss: 5.3313 - acc: 0.648 - ETA: 0s - loss: 5.3013 - acc: 0.650 - ETA: 0s - loss: 5.3224 - acc: 0.648 - ETA: 0s - loss: 5.3374 - acc: 0.648 - ETA: 0s - loss: 5.3134 - acc: 0.649 - ETA: 0s - loss: 5.3482 - acc: 0.647 - ETA: 0s - loss: 5.3630 - acc: 0.646 - ETA: 0s - loss: 5.3706 - acc: 0.646 - ETA: 0s - loss: 5.3925 - acc: 0.645 - ETA: 0s - loss: 5.4309 - acc: 0.643 - ETA: 0s - loss: 5.4546 - acc: 0.641 - ETA: 0s - loss: 5.4822 - acc: 0.639 - ETA: 0s - loss: 5.4631 - acc: 0.640 - ETA: 0s - loss: 5.4870 - acc: 0.638 - ETA: 0s - loss: 5.4933 - acc: 0.639 - ETA: 0s - loss: 5.4972 - acc: 0.638 - ETA: 0s - loss: 5.4845 - acc: 0.639 - 3s 441us/step - loss: 5.4934 - acc: 0.6394 - val_loss: 6.4843 - val_acc: 0.5030\n",
      "\n",
      "Epoch 00014: val_loss improved from 6.59588 to 6.48433, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 4.8365 - acc: 0.700 - ETA: 2s - loss: 4.3835 - acc: 0.728 - ETA: 2s - loss: 4.7975 - acc: 0.696 - ETA: 2s - loss: 5.2136 - acc: 0.672 - ETA: 2s - loss: 5.3582 - acc: 0.664 - ETA: 2s - loss: 5.2326 - acc: 0.667 - ETA: 2s - loss: 5.2671 - acc: 0.665 - ETA: 2s - loss: 5.1952 - acc: 0.670 - ETA: 2s - loss: 5.4032 - acc: 0.657 - ETA: 2s - loss: 5.3117 - acc: 0.662 - ETA: 2s - loss: 5.3492 - acc: 0.660 - ETA: 2s - loss: 5.3514 - acc: 0.659 - ETA: 2s - loss: 5.3545 - acc: 0.659 - ETA: 2s - loss: 5.4150 - acc: 0.654 - ETA: 2s - loss: 5.4154 - acc: 0.654 - ETA: 1s - loss: 5.3830 - acc: 0.656 - ETA: 1s - loss: 5.3683 - acc: 0.657 - ETA: 1s - loss: 5.3403 - acc: 0.657 - ETA: 1s - loss: 5.2687 - acc: 0.662 - ETA: 1s - loss: 5.3083 - acc: 0.658 - ETA: 1s - loss: 5.3324 - acc: 0.657 - ETA: 1s - loss: 5.2796 - acc: 0.661 - ETA: 1s - loss: 5.2343 - acc: 0.663 - ETA: 1s - loss: 5.2781 - acc: 0.661 - ETA: 1s - loss: 5.2188 - acc: 0.665 - ETA: 1s - loss: 5.2122 - acc: 0.664 - ETA: 1s - loss: 5.2464 - acc: 0.662 - ETA: 1s - loss: 5.2636 - acc: 0.660 - ETA: 1s - loss: 5.2562 - acc: 0.660 - ETA: 1s - loss: 5.2477 - acc: 0.661 - ETA: 1s - loss: 5.2687 - acc: 0.659 - ETA: 1s - loss: 5.2598 - acc: 0.659 - ETA: 1s - loss: 5.2346 - acc: 0.660 - ETA: 1s - loss: 5.2857 - acc: 0.657 - ETA: 0s - loss: 5.2875 - acc: 0.657 - ETA: 0s - loss: 5.2906 - acc: 0.657 - ETA: 0s - loss: 5.3096 - acc: 0.655 - ETA: 0s - loss: 5.2908 - acc: 0.656 - ETA: 1s - loss: 5.2862 - acc: 0.656 - ETA: 1s - loss: 5.2876 - acc: 0.656 - ETA: 1s - loss: 5.2859 - acc: 0.656 - ETA: 1s - loss: 5.2819 - acc: 0.657 - ETA: 2s - loss: 5.2739 - acc: 0.657 - ETA: 2s - loss: 5.2755 - acc: 0.657 - ETA: 2s - loss: 5.2665 - acc: 0.657 - ETA: 1s - loss: 5.2480 - acc: 0.658 - ETA: 1s - loss: 5.2679 - acc: 0.657 - ETA: 1s - loss: 5.2792 - acc: 0.656 - ETA: 1s - loss: 5.2832 - acc: 0.656 - ETA: 1s - loss: 5.3240 - acc: 0.653 - ETA: 1s - loss: 5.3516 - acc: 0.652 - ETA: 0s - loss: 5.3536 - acc: 0.652 - ETA: 0s - loss: 5.3711 - acc: 0.651 - ETA: 0s - loss: 5.3765 - acc: 0.650 - ETA: 0s - loss: 5.4016 - acc: 0.648 - ETA: 0s - loss: 5.4101 - acc: 0.648 - ETA: 0s - loss: 5.4125 - acc: 0.648 - ETA: 0s - loss: 5.4161 - acc: 0.648 - 7s 1ms/step - loss: 5.4182 - acc: 0.6479 - val_loss: 6.3807 - val_acc: 0.4970\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.48433 to 6.38073, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 6.4531 - acc: 0.600 - ETA: 2s - loss: 5.7128 - acc: 0.635 - ETA: 2s - loss: 5.8350 - acc: 0.623 - ETA: 2s - loss: 5.6513 - acc: 0.631 - ETA: 2s - loss: 5.4221 - acc: 0.648 - ETA: 2s - loss: 5.5491 - acc: 0.641 - ETA: 2s - loss: 5.3793 - acc: 0.651 - ETA: 2s - loss: 5.3607 - acc: 0.654 - ETA: 2s - loss: 5.3186 - acc: 0.657 - ETA: 2s - loss: 5.2688 - acc: 0.660 - ETA: 2s - loss: 5.2091 - acc: 0.664 - ETA: 2s - loss: 5.2532 - acc: 0.663 - ETA: 2s - loss: 5.1414 - acc: 0.671 - ETA: 2s - loss: 5.2010 - acc: 0.665 - ETA: 2s - loss: 5.2222 - acc: 0.664 - ETA: 2s - loss: 5.2007 - acc: 0.664 - ETA: 1s - loss: 5.2164 - acc: 0.664 - ETA: 1s - loss: 5.2444 - acc: 0.663 - ETA: 1s - loss: 5.2813 - acc: 0.662 - ETA: 1s - loss: 5.3087 - acc: 0.661 - ETA: 1s - loss: 5.3120 - acc: 0.661 - ETA: 1s - loss: 5.3349 - acc: 0.658 - ETA: 1s - loss: 5.2902 - acc: 0.661 - ETA: 1s - loss: 5.2504 - acc: 0.663 - ETA: 1s - loss: 5.2362 - acc: 0.664 - ETA: 1s - loss: 5.2392 - acc: 0.664 - ETA: 1s - loss: 5.2472 - acc: 0.664 - ETA: 1s - loss: 5.2379 - acc: 0.665 - ETA: 1s - loss: 5.2562 - acc: 0.663 - ETA: 1s - loss: 5.2426 - acc: 0.664 - ETA: 1s - loss: 5.2870 - acc: 0.661 - ETA: 1s - loss: 5.2543 - acc: 0.664 - ETA: 1s - loss: 5.2524 - acc: 0.664 - ETA: 1s - loss: 5.2360 - acc: 0.665 - ETA: 0s - loss: 5.2471 - acc: 0.664 - ETA: 0s - loss: 5.2831 - acc: 0.662 - ETA: 0s - loss: 5.2961 - acc: 0.661 - ETA: 0s - loss: 5.3052 - acc: 0.661 - ETA: 0s - loss: 5.2802 - acc: 0.662 - ETA: 0s - loss: 5.3064 - acc: 0.660 - ETA: 0s - loss: 5.3243 - acc: 0.659 - ETA: 0s - loss: 5.3229 - acc: 0.659 - ETA: 0s - loss: 5.3156 - acc: 0.659 - ETA: 0s - loss: 5.3143 - acc: 0.659 - ETA: 0s - loss: 5.3110 - acc: 0.659 - ETA: 0s - loss: 5.3321 - acc: 0.658 - ETA: 0s - loss: 5.3162 - acc: 0.659 - ETA: 0s - loss: 5.3365 - acc: 0.658 - ETA: 0s - loss: 5.3572 - acc: 0.656 - ETA: 0s - loss: 5.3664 - acc: 0.656 - ETA: 0s - loss: 5.3550 - acc: 0.657 - ETA: 0s - loss: 5.3445 - acc: 0.657 - ETA: 0s - loss: 5.3518 - acc: 0.657 - 3s 447us/step - loss: 5.3439 - acc: 0.6576 - val_loss: 6.3697 - val_acc: 0.5174\n",
      "\n",
      "Epoch 00016: val_loss improved from 6.38073 to 6.36974, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.0934 - acc: 0.700 - ETA: 2s - loss: 4.5946 - acc: 0.700 - ETA: 2s - loss: 5.2051 - acc: 0.669 - ETA: 2s - loss: 5.1715 - acc: 0.670 - ETA: 2s - loss: 5.1512 - acc: 0.671 - ETA: 2s - loss: 5.0429 - acc: 0.676 - ETA: 2s - loss: 4.8674 - acc: 0.686 - ETA: 2s - loss: 4.9731 - acc: 0.681 - ETA: 2s - loss: 4.9591 - acc: 0.683 - ETA: 2s - loss: 5.1048 - acc: 0.675 - ETA: 2s - loss: 5.1711 - acc: 0.671 - ETA: 2s - loss: 5.1959 - acc: 0.670 - ETA: 2s - loss: 5.2180 - acc: 0.668 - ETA: 2s - loss: 5.2403 - acc: 0.667 - ETA: 2s - loss: 5.2357 - acc: 0.667 - ETA: 2s - loss: 5.2351 - acc: 0.667 - ETA: 1s - loss: 5.2131 - acc: 0.669 - ETA: 1s - loss: 5.1772 - acc: 0.671 - ETA: 1s - loss: 5.1815 - acc: 0.670 - ETA: 1s - loss: 5.2280 - acc: 0.667 - ETA: 1s - loss: 5.2768 - acc: 0.664 - ETA: 1s - loss: 5.2847 - acc: 0.663 - ETA: 1s - loss: 5.3361 - acc: 0.660 - ETA: 1s - loss: 5.3392 - acc: 0.660 - ETA: 1s - loss: 5.4240 - acc: 0.655 - ETA: 1s - loss: 5.4033 - acc: 0.656 - ETA: 1s - loss: 5.3906 - acc: 0.656 - ETA: 1s - loss: 5.3931 - acc: 0.656 - ETA: 1s - loss: 5.3811 - acc: 0.656 - ETA: 1s - loss: 5.3523 - acc: 0.658 - ETA: 1s - loss: 5.3777 - acc: 0.656 - ETA: 1s - loss: 5.3744 - acc: 0.656 - ETA: 1s - loss: 5.3467 - acc: 0.658 - ETA: 1s - loss: 5.3569 - acc: 0.658 - ETA: 1s - loss: 5.3580 - acc: 0.657 - ETA: 0s - loss: 5.3955 - acc: 0.655 - ETA: 0s - loss: 5.3731 - acc: 0.656 - ETA: 0s - loss: 5.3954 - acc: 0.655 - ETA: 0s - loss: 5.3934 - acc: 0.655 - ETA: 0s - loss: 5.3965 - acc: 0.654 - ETA: 0s - loss: 5.3706 - acc: 0.656 - ETA: 0s - loss: 5.3650 - acc: 0.656 - ETA: 0s - loss: 5.3745 - acc: 0.656 - ETA: 0s - loss: 5.3527 - acc: 0.657 - ETA: 0s - loss: 5.3657 - acc: 0.656 - ETA: 0s - loss: 5.3732 - acc: 0.656 - ETA: 0s - loss: 5.3678 - acc: 0.656 - ETA: 0s - loss: 5.3600 - acc: 0.657 - ETA: 0s - loss: 5.3584 - acc: 0.657 - ETA: 0s - loss: 5.3437 - acc: 0.658 - ETA: 0s - loss: 5.3164 - acc: 0.659 - ETA: 0s - loss: 5.3189 - acc: 0.659 - ETA: 0s - loss: 5.3069 - acc: 0.660 - ETA: 0s - loss: 5.3140 - acc: 0.659 - 3s 452us/step - loss: 5.3112 - acc: 0.6597 - val_loss: 6.4097 - val_acc: 0.5150\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 19s - loss: 7.2538 - acc: 0.55 - ETA: 5s - loss: 4.8459 - acc: 0.6929 - ETA: 4s - loss: 4.7010 - acc: 0.700 - ETA: 3s - loss: 4.7048 - acc: 0.702 - ETA: 3s - loss: 4.6089 - acc: 0.710 - ETA: 3s - loss: 4.7258 - acc: 0.700 - ETA: 3s - loss: 4.9492 - acc: 0.682 - ETA: 2s - loss: 5.0467 - acc: 0.677 - ETA: 2s - loss: 5.1213 - acc: 0.673 - ETA: 2s - loss: 5.0714 - acc: 0.677 - ETA: 2s - loss: 5.1014 - acc: 0.676 - ETA: 2s - loss: 5.2340 - acc: 0.668 - ETA: 2s - loss: 5.1928 - acc: 0.671 - ETA: 2s - loss: 5.2702 - acc: 0.666 - ETA: 2s - loss: 5.3101 - acc: 0.664 - ETA: 2s - loss: 5.2790 - acc: 0.666 - ETA: 2s - loss: 5.2868 - acc: 0.665 - ETA: 2s - loss: 5.3014 - acc: 0.664 - ETA: 2s - loss: 5.2833 - acc: 0.665 - ETA: 2s - loss: 5.2022 - acc: 0.670 - ETA: 1s - loss: 5.2732 - acc: 0.666 - ETA: 1s - loss: 5.2389 - acc: 0.667 - ETA: 1s - loss: 5.2413 - acc: 0.667 - ETA: 1s - loss: 5.2521 - acc: 0.666 - ETA: 1s - loss: 5.2586 - acc: 0.664 - ETA: 1s - loss: 5.2729 - acc: 0.663 - ETA: 1s - loss: 5.2926 - acc: 0.661 - ETA: 1s - loss: 5.2701 - acc: 0.663 - ETA: 1s - loss: 5.2902 - acc: 0.661 - ETA: 1s - loss: 5.3061 - acc: 0.660 - ETA: 1s - loss: 5.2691 - acc: 0.662 - ETA: 1s - loss: 5.2813 - acc: 0.661 - ETA: 1s - loss: 5.2926 - acc: 0.661 - ETA: 1s - loss: 5.3014 - acc: 0.660 - ETA: 1s - loss: 5.3018 - acc: 0.658 - ETA: 1s - loss: 5.3285 - acc: 0.657 - ETA: 1s - loss: 5.3562 - acc: 0.655 - ETA: 0s - loss: 5.3276 - acc: 0.658 - ETA: 0s - loss: 5.3282 - acc: 0.657 - ETA: 0s - loss: 5.3284 - acc: 0.657 - ETA: 0s - loss: 5.2940 - acc: 0.659 - ETA: 0s - loss: 5.3114 - acc: 0.658 - ETA: 0s - loss: 5.2947 - acc: 0.659 - ETA: 0s - loss: 5.2735 - acc: 0.660 - ETA: 0s - loss: 5.2427 - acc: 0.662 - ETA: 0s - loss: 5.2380 - acc: 0.662 - ETA: 0s - loss: 5.2391 - acc: 0.662 - ETA: 0s - loss: 5.2285 - acc: 0.663 - ETA: 0s - loss: 5.1901 - acc: 0.665 - ETA: 0s - loss: 5.1914 - acc: 0.665 - ETA: 0s - loss: 5.2013 - acc: 0.664 - ETA: 0s - loss: 5.1970 - acc: 0.665 - ETA: 0s - loss: 5.2107 - acc: 0.663 - ETA: 0s - loss: 5.2224 - acc: 0.663 - ETA: 0s - loss: 5.2168 - acc: 0.663 - 3s 468us/step - loss: 5.1901 - acc: 0.6650 - val_loss: 6.2385 - val_acc: 0.5162\n",
      "\n",
      "Epoch 00018: val_loss improved from 6.36974 to 6.23855, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 11s - loss: 6.4478 - acc: 0.60 - ETA: 4s - loss: 4.4926 - acc: 0.7214 - ETA: 3s - loss: 4.3543 - acc: 0.723 - ETA: 3s - loss: 3.8593 - acc: 0.752 - ETA: 3s - loss: 4.3840 - acc: 0.722 - ETA: 2s - loss: 4.6072 - acc: 0.704 - ETA: 2s - loss: 4.7757 - acc: 0.695 - ETA: 2s - loss: 4.7099 - acc: 0.701 - ETA: 2s - loss: 4.7269 - acc: 0.698 - ETA: 2s - loss: 4.6609 - acc: 0.701 - ETA: 2s - loss: 4.5934 - acc: 0.706 - ETA: 2s - loss: 4.6993 - acc: 0.700 - ETA: 2s - loss: 4.8327 - acc: 0.692 - ETA: 2s - loss: 4.8942 - acc: 0.689 - ETA: 2s - loss: 4.8660 - acc: 0.690 - ETA: 2s - loss: 4.9403 - acc: 0.685 - ETA: 2s - loss: 4.9415 - acc: 0.684 - ETA: 2s - loss: 4.9194 - acc: 0.684 - ETA: 2s - loss: 4.8413 - acc: 0.689 - ETA: 1s - loss: 4.8577 - acc: 0.688 - ETA: 1s - loss: 4.9032 - acc: 0.686 - ETA: 1s - loss: 4.8678 - acc: 0.688 - ETA: 1s - loss: 4.8308 - acc: 0.690 - ETA: 1s - loss: 4.8136 - acc: 0.692 - ETA: 1s - loss: 4.7873 - acc: 0.694 - ETA: 1s - loss: 4.7915 - acc: 0.693 - ETA: 1s - loss: 4.8316 - acc: 0.691 - ETA: 1s - loss: 4.8666 - acc: 0.689 - ETA: 1s - loss: 4.8719 - acc: 0.688 - ETA: 1s - loss: 4.8734 - acc: 0.688 - ETA: 1s - loss: 4.8126 - acc: 0.692 - ETA: 1s - loss: 4.8395 - acc: 0.691 - ETA: 1s - loss: 4.8593 - acc: 0.689 - ETA: 1s - loss: 4.8628 - acc: 0.689 - ETA: 1s - loss: 4.8978 - acc: 0.686 - ETA: 1s - loss: 4.9391 - acc: 0.684 - ETA: 0s - loss: 4.9655 - acc: 0.682 - ETA: 0s - loss: 4.9419 - acc: 0.684 - ETA: 0s - loss: 4.9316 - acc: 0.684 - ETA: 0s - loss: 4.9923 - acc: 0.681 - ETA: 0s - loss: 4.9646 - acc: 0.682 - ETA: 0s - loss: 4.9736 - acc: 0.681 - ETA: 0s - loss: 4.9455 - acc: 0.682 - ETA: 0s - loss: 4.9386 - acc: 0.682 - ETA: 0s - loss: 4.9573 - acc: 0.681 - ETA: 0s - loss: 4.9737 - acc: 0.681 - ETA: 0s - loss: 4.9641 - acc: 0.681 - ETA: 0s - loss: 5.0014 - acc: 0.679 - ETA: 0s - loss: 5.0061 - acc: 0.678 - ETA: 0s - loss: 4.9976 - acc: 0.678 - ETA: 0s - loss: 5.0189 - acc: 0.676 - ETA: 0s - loss: 5.0737 - acc: 0.673 - ETA: 0s - loss: 5.0757 - acc: 0.673 - ETA: 0s - loss: 5.0677 - acc: 0.673 - ETA: 0s - loss: 5.0649 - acc: 0.673 - 3s 463us/step - loss: 5.0642 - acc: 0.6732 - val_loss: 6.1498 - val_acc: 0.5090\n",
      "\n",
      "Epoch 00019: val_loss improved from 6.23855 to 6.14983, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.2240 - acc: 0.800 - ETA: 2s - loss: 3.8038 - acc: 0.764 - ETA: 2s - loss: 4.5354 - acc: 0.715 - ETA: 2s - loss: 4.3697 - acc: 0.718 - ETA: 2s - loss: 4.5649 - acc: 0.706 - ETA: 2s - loss: 4.7789 - acc: 0.691 - ETA: 2s - loss: 4.7658 - acc: 0.689 - ETA: 2s - loss: 4.7311 - acc: 0.691 - ETA: 2s - loss: 4.9087 - acc: 0.682 - ETA: 2s - loss: 4.8530 - acc: 0.686 - ETA: 2s - loss: 4.8674 - acc: 0.683 - ETA: 2s - loss: 4.9333 - acc: 0.679 - ETA: 2s - loss: 4.9713 - acc: 0.676 - ETA: 2s - loss: 4.9374 - acc: 0.677 - ETA: 2s - loss: 5.0075 - acc: 0.673 - ETA: 2s - loss: 4.9766 - acc: 0.675 - ETA: 2s - loss: 5.0490 - acc: 0.671 - ETA: 1s - loss: 4.9504 - acc: 0.678 - ETA: 1s - loss: 4.9880 - acc: 0.675 - ETA: 1s - loss: 5.0148 - acc: 0.672 - ETA: 1s - loss: 5.0387 - acc: 0.671 - ETA: 1s - loss: 5.0134 - acc: 0.673 - ETA: 1s - loss: 5.0028 - acc: 0.674 - ETA: 1s - loss: 5.0294 - acc: 0.672 - ETA: 1s - loss: 4.9634 - acc: 0.676 - ETA: 1s - loss: 4.9552 - acc: 0.677 - ETA: 1s - loss: 4.9491 - acc: 0.678 - ETA: 1s - loss: 4.9435 - acc: 0.678 - ETA: 1s - loss: 4.9447 - acc: 0.678 - ETA: 1s - loss: 4.8916 - acc: 0.682 - ETA: 1s - loss: 4.9167 - acc: 0.680 - ETA: 1s - loss: 4.8921 - acc: 0.681 - ETA: 1s - loss: 4.8720 - acc: 0.683 - ETA: 1s - loss: 4.8687 - acc: 0.683 - ETA: 1s - loss: 4.8567 - acc: 0.684 - ETA: 1s - loss: 4.8755 - acc: 0.683 - ETA: 1s - loss: 4.8533 - acc: 0.685 - ETA: 0s - loss: 4.8789 - acc: 0.683 - ETA: 0s - loss: 4.8856 - acc: 0.683 - ETA: 0s - loss: 4.9193 - acc: 0.681 - ETA: 0s - loss: 4.8839 - acc: 0.683 - ETA: 0s - loss: 4.8928 - acc: 0.682 - ETA: 0s - loss: 4.8914 - acc: 0.683 - ETA: 0s - loss: 4.8804 - acc: 0.683 - ETA: 0s - loss: 4.9056 - acc: 0.682 - ETA: 0s - loss: 4.8769 - acc: 0.683 - ETA: 0s - loss: 4.8894 - acc: 0.682 - ETA: 0s - loss: 4.9095 - acc: 0.681 - ETA: 0s - loss: 4.9298 - acc: 0.681 - ETA: 0s - loss: 4.9078 - acc: 0.682 - ETA: 0s - loss: 4.9196 - acc: 0.681 - ETA: 0s - loss: 4.9127 - acc: 0.682 - ETA: 0s - loss: 4.9178 - acc: 0.681 - ETA: 0s - loss: 4.9567 - acc: 0.679 - ETA: 0s - loss: 4.9612 - acc: 0.679 - 3s 453us/step - loss: 4.9609 - acc: 0.6793 - val_loss: 6.1988 - val_acc: 0.5162\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3fcb771f60>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 52.9904%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "\n",
    "bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_Resnet50 = bottleneck_features['train']\n",
    "valid_Resnet50 = bottleneck_features['valid']\n",
    "test_Resnet50 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "The CNN architecture for this model consists of pre-trained Resnet50 network and a GlobalAveragePooling2D layer as input. The output layer is a fully connected newtwork with 133 nodes and softmax activation. Total number of parameters are 272,517\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_12  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "Resnet50_model = Sequential()\n",
    "\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_Resnet50.shape[1:]))\n",
    "Resnet50_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 12:08 - loss: 5.5712 - acc: 0.0000e+ - ETA: 1:12 - loss: 5.7771 - acc: 0.0250     - ETA: 40s - loss: 5.3450 - acc: 0.069 - ETA: 27s - loss: 4.9442 - acc: 0.10 - ETA: 20s - loss: 4.5532 - acc: 0.16 - ETA: 16s - loss: 4.2075 - acc: 0.20 - ETA: 13s - loss: 3.9833 - acc: 0.22 - ETA: 11s - loss: 3.7295 - acc: 0.26 - ETA: 9s - loss: 3.5131 - acc: 0.2937 - ETA: 8s - loss: 3.3435 - acc: 0.316 - ETA: 7s - loss: 3.1819 - acc: 0.345 - ETA: 6s - loss: 3.0482 - acc: 0.359 - ETA: 5s - loss: 2.9157 - acc: 0.381 - ETA: 5s - loss: 2.8053 - acc: 0.399 - ETA: 4s - loss: 2.7037 - acc: 0.416 - ETA: 4s - loss: 2.5973 - acc: 0.432 - ETA: 4s - loss: 2.5208 - acc: 0.442 - ETA: 3s - loss: 2.4384 - acc: 0.458 - ETA: 3s - loss: 2.3573 - acc: 0.473 - ETA: 3s - loss: 2.2932 - acc: 0.482 - ETA: 2s - loss: 2.2337 - acc: 0.493 - ETA: 2s - loss: 2.1773 - acc: 0.502 - ETA: 2s - loss: 2.1204 - acc: 0.514 - ETA: 2s - loss: 2.0752 - acc: 0.523 - ETA: 1s - loss: 2.0312 - acc: 0.531 - ETA: 1s - loss: 1.9950 - acc: 0.535 - ETA: 1s - loss: 1.9562 - acc: 0.543 - ETA: 1s - loss: 1.9197 - acc: 0.549 - ETA: 1s - loss: 1.8790 - acc: 0.556 - ETA: 1s - loss: 1.8421 - acc: 0.562 - ETA: 0s - loss: 1.8161 - acc: 0.564 - ETA: 0s - loss: 1.7834 - acc: 0.571 - ETA: 0s - loss: 1.7540 - acc: 0.576 - ETA: 0s - loss: 1.7267 - acc: 0.580 - ETA: 0s - loss: 1.7035 - acc: 0.584 - ETA: 0s - loss: 1.6748 - acc: 0.589 - ETA: 0s - loss: 1.6477 - acc: 0.595 - ETA: 0s - loss: 1.6238 - acc: 0.599 - 4s 653us/step - loss: 1.6236 - acc: 0.5996 - val_loss: 0.8174 - val_acc: 0.7413\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81736, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.4368 - acc: 0.850 - ETA: 2s - loss: 0.4153 - acc: 0.900 - ETA: 2s - loss: 0.4794 - acc: 0.873 - ETA: 1s - loss: 0.4436 - acc: 0.882 - ETA: 1s - loss: 0.4144 - acc: 0.886 - ETA: 1s - loss: 0.4003 - acc: 0.887 - ETA: 1s - loss: 0.3979 - acc: 0.886 - ETA: 1s - loss: 0.4080 - acc: 0.875 - ETA: 1s - loss: 0.4233 - acc: 0.870 - ETA: 1s - loss: 0.4298 - acc: 0.870 - ETA: 1s - loss: 0.4192 - acc: 0.872 - ETA: 1s - loss: 0.4225 - acc: 0.871 - ETA: 1s - loss: 0.4288 - acc: 0.869 - ETA: 1s - loss: 0.4324 - acc: 0.868 - ETA: 1s - loss: 0.4310 - acc: 0.870 - ETA: 1s - loss: 0.4362 - acc: 0.869 - ETA: 1s - loss: 0.4356 - acc: 0.867 - ETA: 1s - loss: 0.4348 - acc: 0.868 - ETA: 1s - loss: 0.4365 - acc: 0.867 - ETA: 1s - loss: 0.4368 - acc: 0.866 - ETA: 0s - loss: 0.4364 - acc: 0.866 - ETA: 0s - loss: 0.4376 - acc: 0.865 - ETA: 0s - loss: 0.4303 - acc: 0.867 - ETA: 0s - loss: 0.4318 - acc: 0.868 - ETA: 0s - loss: 0.4325 - acc: 0.867 - ETA: 0s - loss: 0.4324 - acc: 0.869 - ETA: 0s - loss: 0.4310 - acc: 0.870 - ETA: 0s - loss: 0.4319 - acc: 0.870 - ETA: 0s - loss: 0.4371 - acc: 0.868 - ETA: 0s - loss: 0.4348 - acc: 0.869 - ETA: 0s - loss: 0.4342 - acc: 0.868 - ETA: 0s - loss: 0.4331 - acc: 0.869 - ETA: 0s - loss: 0.4301 - acc: 0.869 - ETA: 0s - loss: 0.4288 - acc: 0.869 - ETA: 0s - loss: 0.4297 - acc: 0.868 - ETA: 0s - loss: 0.4315 - acc: 0.868 - ETA: 0s - loss: 0.4303 - acc: 0.868 - 2s 304us/step - loss: 0.4293 - acc: 0.8687 - val_loss: 0.7583 - val_acc: 0.7737\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81736 to 0.75833, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 12s - loss: 0.3560 - acc: 0.85 - ETA: 3s - loss: 0.2208 - acc: 0.9389 - ETA: 2s - loss: 0.2223 - acc: 0.933 - ETA: 2s - loss: 0.2182 - acc: 0.926 - ETA: 2s - loss: 0.2251 - acc: 0.925 - ETA: 2s - loss: 0.2229 - acc: 0.926 - ETA: 1s - loss: 0.2133 - acc: 0.930 - ETA: 1s - loss: 0.2311 - acc: 0.922 - ETA: 1s - loss: 0.2275 - acc: 0.923 - ETA: 1s - loss: 0.2250 - acc: 0.923 - ETA: 1s - loss: 0.2331 - acc: 0.920 - ETA: 1s - loss: 0.2390 - acc: 0.918 - ETA: 1s - loss: 0.2398 - acc: 0.916 - ETA: 1s - loss: 0.2448 - acc: 0.917 - ETA: 1s - loss: 0.2443 - acc: 0.917 - ETA: 1s - loss: 0.2458 - acc: 0.918 - ETA: 1s - loss: 0.2501 - acc: 0.918 - ETA: 1s - loss: 0.2496 - acc: 0.918 - ETA: 1s - loss: 0.2463 - acc: 0.920 - ETA: 0s - loss: 0.2427 - acc: 0.921 - ETA: 0s - loss: 0.2443 - acc: 0.921 - ETA: 0s - loss: 0.2452 - acc: 0.921 - ETA: 0s - loss: 0.2465 - acc: 0.920 - ETA: 0s - loss: 0.2450 - acc: 0.921 - ETA: 0s - loss: 0.2440 - acc: 0.921 - ETA: 0s - loss: 0.2481 - acc: 0.920 - ETA: 0s - loss: 0.2484 - acc: 0.920 - ETA: 0s - loss: 0.2477 - acc: 0.920 - ETA: 0s - loss: 0.2481 - acc: 0.920 - ETA: 0s - loss: 0.2474 - acc: 0.921 - ETA: 0s - loss: 0.2485 - acc: 0.920 - ETA: 0s - loss: 0.2533 - acc: 0.919 - ETA: 0s - loss: 0.2529 - acc: 0.919 - ETA: 0s - loss: 0.2519 - acc: 0.919 - ETA: 0s - loss: 0.2525 - acc: 0.919 - ETA: 0s - loss: 0.2568 - acc: 0.918 - 2s 298us/step - loss: 0.2605 - acc: 0.9183 - val_loss: 0.6672 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75833 to 0.66724, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0181 - acc: 1.000 - ETA: 1s - loss: 0.0841 - acc: 0.980 - ETA: 1s - loss: 0.1237 - acc: 0.965 - ETA: 1s - loss: 0.1177 - acc: 0.971 - ETA: 1s - loss: 0.1209 - acc: 0.968 - ETA: 1s - loss: 0.1197 - acc: 0.968 - ETA: 1s - loss: 0.1377 - acc: 0.959 - ETA: 1s - loss: 0.1356 - acc: 0.960 - ETA: 1s - loss: 0.1359 - acc: 0.960 - ETA: 1s - loss: 0.1307 - acc: 0.962 - ETA: 1s - loss: 0.1354 - acc: 0.960 - ETA: 1s - loss: 0.1375 - acc: 0.960 - ETA: 1s - loss: 0.1371 - acc: 0.959 - ETA: 1s - loss: 0.1398 - acc: 0.957 - ETA: 1s - loss: 0.1421 - acc: 0.955 - ETA: 1s - loss: 0.1432 - acc: 0.956 - ETA: 1s - loss: 0.1445 - acc: 0.955 - ETA: 0s - loss: 0.1501 - acc: 0.952 - ETA: 0s - loss: 0.1524 - acc: 0.951 - ETA: 0s - loss: 0.1570 - acc: 0.950 - ETA: 0s - loss: 0.1595 - acc: 0.949 - ETA: 0s - loss: 0.1589 - acc: 0.949 - ETA: 0s - loss: 0.1637 - acc: 0.948 - ETA: 0s - loss: 0.1650 - acc: 0.948 - ETA: 0s - loss: 0.1654 - acc: 0.948 - ETA: 0s - loss: 0.1663 - acc: 0.947 - ETA: 0s - loss: 0.1653 - acc: 0.947 - ETA: 0s - loss: 0.1662 - acc: 0.947 - ETA: 0s - loss: 0.1696 - acc: 0.947 - ETA: 0s - loss: 0.1679 - acc: 0.947 - ETA: 0s - loss: 0.1698 - acc: 0.946 - ETA: 0s - loss: 0.1678 - acc: 0.947 - ETA: 0s - loss: 0.1666 - acc: 0.948 - ETA: 0s - loss: 0.1677 - acc: 0.947 - ETA: 0s - loss: 0.1697 - acc: 0.946 - ETA: 0s - loss: 0.1707 - acc: 0.946 - 2s 292us/step - loss: 0.1728 - acc: 0.9454 - val_loss: 0.7358 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.2816 - acc: 0.850 - ETA: 1s - loss: 0.1096 - acc: 0.959 - ETA: 1s - loss: 0.0808 - acc: 0.976 - ETA: 1s - loss: 0.0933 - acc: 0.971 - ETA: 1s - loss: 0.1034 - acc: 0.965 - ETA: 1s - loss: 0.0977 - acc: 0.967 - ETA: 1s - loss: 0.1047 - acc: 0.967 - ETA: 1s - loss: 0.1091 - acc: 0.966 - ETA: 1s - loss: 0.1080 - acc: 0.967 - ETA: 1s - loss: 0.1083 - acc: 0.969 - ETA: 1s - loss: 0.1048 - acc: 0.970 - ETA: 1s - loss: 0.1068 - acc: 0.971 - ETA: 1s - loss: 0.1025 - acc: 0.973 - ETA: 1s - loss: 0.1045 - acc: 0.973 - ETA: 1s - loss: 0.1050 - acc: 0.972 - ETA: 1s - loss: 0.1040 - acc: 0.971 - ETA: 0s - loss: 0.1079 - acc: 0.970 - ETA: 0s - loss: 0.1065 - acc: 0.970 - ETA: 0s - loss: 0.1078 - acc: 0.970 - ETA: 0s - loss: 0.1071 - acc: 0.971 - ETA: 0s - loss: 0.1090 - acc: 0.971 - ETA: 0s - loss: 0.1096 - acc: 0.970 - ETA: 0s - loss: 0.1132 - acc: 0.969 - ETA: 0s - loss: 0.1116 - acc: 0.969 - ETA: 0s - loss: 0.1133 - acc: 0.969 - ETA: 0s - loss: 0.1132 - acc: 0.969 - ETA: 0s - loss: 0.1125 - acc: 0.969 - ETA: 0s - loss: 0.1139 - acc: 0.968 - ETA: 0s - loss: 0.1118 - acc: 0.969 - ETA: 0s - loss: 0.1128 - acc: 0.969 - ETA: 0s - loss: 0.1139 - acc: 0.968 - ETA: 0s - loss: 0.1130 - acc: 0.968 - ETA: 0s - loss: 0.1157 - acc: 0.967 - ETA: 0s - loss: 0.1159 - acc: 0.967 - ETA: 0s - loss: 0.1167 - acc: 0.967 - 2s 285us/step - loss: 0.1179 - acc: 0.9666 - val_loss: 0.6961 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0055 - acc: 1.000 - ETA: 1s - loss: 0.0663 - acc: 0.986 - ETA: 1s - loss: 0.0617 - acc: 0.987 - ETA: 1s - loss: 0.0686 - acc: 0.984 - ETA: 1s - loss: 0.0677 - acc: 0.985 - ETA: 1s - loss: 0.0609 - acc: 0.987 - ETA: 1s - loss: 0.0560 - acc: 0.988 - ETA: 1s - loss: 0.0606 - acc: 0.985 - ETA: 1s - loss: 0.0586 - acc: 0.985 - ETA: 1s - loss: 0.0599 - acc: 0.985 - ETA: 1s - loss: 0.0613 - acc: 0.984 - ETA: 1s - loss: 0.0591 - acc: 0.985 - ETA: 1s - loss: 0.0653 - acc: 0.983 - ETA: 1s - loss: 0.0651 - acc: 0.983 - ETA: 1s - loss: 0.0643 - acc: 0.983 - ETA: 1s - loss: 0.0661 - acc: 0.983 - ETA: 1s - loss: 0.0680 - acc: 0.982 - ETA: 0s - loss: 0.0720 - acc: 0.980 - ETA: 0s - loss: 0.0736 - acc: 0.979 - ETA: 0s - loss: 0.0784 - acc: 0.978 - ETA: 0s - loss: 0.0796 - acc: 0.977 - ETA: 0s - loss: 0.0817 - acc: 0.976 - ETA: 0s - loss: 0.0820 - acc: 0.976 - ETA: 0s - loss: 0.0835 - acc: 0.975 - ETA: 0s - loss: 0.0853 - acc: 0.974 - ETA: 0s - loss: 0.0865 - acc: 0.973 - ETA: 0s - loss: 0.0863 - acc: 0.973 - ETA: 0s - loss: 0.0854 - acc: 0.973 - ETA: 0s - loss: 0.0850 - acc: 0.973 - ETA: 0s - loss: 0.0861 - acc: 0.972 - ETA: 0s - loss: 0.0880 - acc: 0.972 - ETA: 0s - loss: 0.0892 - acc: 0.971 - ETA: 0s - loss: 0.0883 - acc: 0.972 - ETA: 0s - loss: 0.0887 - acc: 0.971 - ETA: 0s - loss: 0.0871 - acc: 0.972 - ETA: 0s - loss: 0.0874 - acc: 0.971 - ETA: 0s - loss: 0.0875 - acc: 0.971 - ETA: 0s - loss: 0.0866 - acc: 0.971 - 2s 310us/step - loss: 0.0864 - acc: 0.9714 - val_loss: 0.7059 - val_acc: 0.8024\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.2031 - acc: 0.900 - ETA: 1s - loss: 0.0623 - acc: 0.975 - ETA: 1s - loss: 0.0429 - acc: 0.986 - ETA: 1s - loss: 0.0508 - acc: 0.982 - ETA: 1s - loss: 0.0475 - acc: 0.983 - ETA: 1s - loss: 0.0539 - acc: 0.982 - ETA: 1s - loss: 0.0496 - acc: 0.984 - ETA: 1s - loss: 0.0477 - acc: 0.985 - ETA: 1s - loss: 0.0461 - acc: 0.985 - ETA: 1s - loss: 0.0501 - acc: 0.984 - ETA: 1s - loss: 0.0500 - acc: 0.984 - ETA: 1s - loss: 0.0485 - acc: 0.985 - ETA: 1s - loss: 0.0494 - acc: 0.984 - ETA: 1s - loss: 0.0522 - acc: 0.983 - ETA: 1s - loss: 0.0528 - acc: 0.983 - ETA: 1s - loss: 0.0526 - acc: 0.984 - ETA: 1s - loss: 0.0517 - acc: 0.985 - ETA: 1s - loss: 0.0545 - acc: 0.985 - ETA: 1s - loss: 0.0558 - acc: 0.984 - ETA: 1s - loss: 0.0563 - acc: 0.984 - ETA: 1s - loss: 0.0565 - acc: 0.984 - ETA: 0s - loss: 0.0568 - acc: 0.984 - ETA: 0s - loss: 0.0578 - acc: 0.983 - ETA: 0s - loss: 0.0592 - acc: 0.982 - ETA: 0s - loss: 0.0581 - acc: 0.983 - ETA: 0s - loss: 0.0574 - acc: 0.983 - ETA: 0s - loss: 0.0570 - acc: 0.983 - ETA: 0s - loss: 0.0574 - acc: 0.983 - ETA: 0s - loss: 0.0575 - acc: 0.983 - ETA: 0s - loss: 0.0579 - acc: 0.983 - ETA: 0s - loss: 0.0573 - acc: 0.983 - ETA: 0s - loss: 0.0582 - acc: 0.983 - ETA: 0s - loss: 0.0601 - acc: 0.982 - ETA: 0s - loss: 0.0613 - acc: 0.982 - ETA: 0s - loss: 0.0621 - acc: 0.982 - ETA: 0s - loss: 0.0622 - acc: 0.982 - ETA: 0s - loss: 0.0624 - acc: 0.981 - ETA: 0s - loss: 0.0620 - acc: 0.982 - ETA: 0s - loss: 0.0650 - acc: 0.981 - ETA: 0s - loss: 0.0653 - acc: 0.980 - 2s 327us/step - loss: 0.0656 - acc: 0.9810 - val_loss: 0.6953 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0073 - acc: 1.000 - ETA: 1s - loss: 0.0305 - acc: 0.995 - ETA: 1s - loss: 0.0278 - acc: 0.997 - ETA: 1s - loss: 0.0287 - acc: 0.994 - ETA: 1s - loss: 0.0293 - acc: 0.992 - ETA: 1s - loss: 0.0334 - acc: 0.991 - ETA: 1s - loss: 0.0350 - acc: 0.990 - ETA: 1s - loss: 0.0362 - acc: 0.990 - ETA: 1s - loss: 0.0361 - acc: 0.989 - ETA: 1s - loss: 0.0385 - acc: 0.988 - ETA: 1s - loss: 0.0383 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.989 - ETA: 1s - loss: 0.0361 - acc: 0.989 - ETA: 1s - loss: 0.0361 - acc: 0.989 - ETA: 1s - loss: 0.0352 - acc: 0.990 - ETA: 1s - loss: 0.0374 - acc: 0.989 - ETA: 1s - loss: 0.0373 - acc: 0.990 - ETA: 1s - loss: 0.0365 - acc: 0.990 - ETA: 1s - loss: 0.0365 - acc: 0.990 - ETA: 1s - loss: 0.0373 - acc: 0.990 - ETA: 0s - loss: 0.0367 - acc: 0.989 - ETA: 0s - loss: 0.0363 - acc: 0.989 - ETA: 0s - loss: 0.0350 - acc: 0.990 - ETA: 0s - loss: 0.0361 - acc: 0.990 - ETA: 0s - loss: 0.0364 - acc: 0.990 - ETA: 0s - loss: 0.0375 - acc: 0.990 - ETA: 0s - loss: 0.0380 - acc: 0.989 - ETA: 0s - loss: 0.0387 - acc: 0.989 - ETA: 0s - loss: 0.0396 - acc: 0.989 - ETA: 0s - loss: 0.0392 - acc: 0.989 - ETA: 0s - loss: 0.0395 - acc: 0.989 - ETA: 0s - loss: 0.0404 - acc: 0.988 - ETA: 0s - loss: 0.0425 - acc: 0.987 - ETA: 0s - loss: 0.0452 - acc: 0.987 - ETA: 0s - loss: 0.0457 - acc: 0.987 - ETA: 0s - loss: 0.0456 - acc: 0.987 - ETA: 0s - loss: 0.0462 - acc: 0.987 - 2s 298us/step - loss: 0.0463 - acc: 0.9870 - val_loss: 0.7407 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0293 - acc: 1.000 - ETA: 1s - loss: 0.0170 - acc: 1.000 - ETA: 1s - loss: 0.0214 - acc: 0.997 - ETA: 1s - loss: 0.0363 - acc: 0.994 - ETA: 1s - loss: 0.0326 - acc: 0.994 - ETA: 1s - loss: 0.0304 - acc: 0.993 - ETA: 1s - loss: 0.0318 - acc: 0.992 - ETA: 1s - loss: 0.0369 - acc: 0.989 - ETA: 1s - loss: 0.0360 - acc: 0.989 - ETA: 1s - loss: 0.0362 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0329 - acc: 0.990 - ETA: 1s - loss: 0.0321 - acc: 0.990 - ETA: 1s - loss: 0.0311 - acc: 0.990 - ETA: 1s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0327 - acc: 0.990 - ETA: 0s - loss: 0.0323 - acc: 0.990 - ETA: 0s - loss: 0.0319 - acc: 0.990 - ETA: 0s - loss: 0.0320 - acc: 0.990 - ETA: 0s - loss: 0.0313 - acc: 0.991 - ETA: 0s - loss: 0.0310 - acc: 0.991 - ETA: 0s - loss: 0.0320 - acc: 0.991 - ETA: 0s - loss: 0.0318 - acc: 0.991 - ETA: 0s - loss: 0.0314 - acc: 0.991 - ETA: 0s - loss: 0.0333 - acc: 0.990 - ETA: 0s - loss: 0.0334 - acc: 0.990 - ETA: 0s - loss: 0.0345 - acc: 0.990 - ETA: 0s - loss: 0.0347 - acc: 0.990 - ETA: 0s - loss: 0.0349 - acc: 0.990 - ETA: 0s - loss: 0.0354 - acc: 0.989 - ETA: 0s - loss: 0.0353 - acc: 0.989 - ETA: 0s - loss: 0.0356 - acc: 0.989 - ETA: 0s - loss: 0.0355 - acc: 0.989 - ETA: 0s - loss: 0.0356 - acc: 0.989 - ETA: 0s - loss: 0.0354 - acc: 0.989 - 2s 283us/step - loss: 0.0354 - acc: 0.9895 - val_loss: 0.7242 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0018 - acc: 1.000 - ETA: 1s - loss: 0.0168 - acc: 1.000 - ETA: 1s - loss: 0.0175 - acc: 0.997 - ETA: 1s - loss: 0.0169 - acc: 0.998 - ETA: 1s - loss: 0.0214 - acc: 0.997 - ETA: 1s - loss: 0.0184 - acc: 0.997 - ETA: 1s - loss: 0.0214 - acc: 0.997 - ETA: 1s - loss: 0.0207 - acc: 0.997 - ETA: 1s - loss: 0.0198 - acc: 0.998 - ETA: 1s - loss: 0.0228 - acc: 0.997 - ETA: 1s - loss: 0.0233 - acc: 0.996 - ETA: 1s - loss: 0.0236 - acc: 0.995 - ETA: 1s - loss: 0.0249 - acc: 0.995 - ETA: 1s - loss: 0.0243 - acc: 0.995 - ETA: 1s - loss: 0.0254 - acc: 0.994 - ETA: 1s - loss: 0.0248 - acc: 0.994 - ETA: 1s - loss: 0.0240 - acc: 0.995 - ETA: 0s - loss: 0.0238 - acc: 0.995 - ETA: 0s - loss: 0.0236 - acc: 0.995 - ETA: 0s - loss: 0.0231 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.995 - ETA: 0s - loss: 0.0238 - acc: 0.995 - ETA: 0s - loss: 0.0233 - acc: 0.995 - ETA: 0s - loss: 0.0231 - acc: 0.995 - ETA: 0s - loss: 0.0236 - acc: 0.994 - ETA: 0s - loss: 0.0232 - acc: 0.994 - ETA: 0s - loss: 0.0236 - acc: 0.994 - ETA: 0s - loss: 0.0235 - acc: 0.994 - ETA: 0s - loss: 0.0243 - acc: 0.994 - ETA: 0s - loss: 0.0253 - acc: 0.994 - ETA: 0s - loss: 0.0256 - acc: 0.994 - ETA: 0s - loss: 0.0255 - acc: 0.994 - ETA: 0s - loss: 0.0265 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0265 - acc: 0.993 - 2s 284us/step - loss: 0.0265 - acc: 0.9934 - val_loss: 0.7486 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1315 - acc: 0.950 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.994 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0210 - acc: 0.994 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0208 - acc: 0.994 - 2s 290us/step - loss: 0.0210 - acc: 0.9942 - val_loss: 0.7933 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 7.2163e-04 - acc: 1.000 - ETA: 1s - loss: 0.0070 - acc: 1.0000    - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0153 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - 2s 297us/step - loss: 0.0159 - acc: 0.9957 - val_loss: 0.8046 - val_acc: 0.8228\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0039 - acc: 1.000 - ETA: 1s - loss: 0.0043 - acc: 1.000 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0102 - acc: 0.997 - ETA: 1s - loss: 0.0102 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.997 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - 2s 288us/step - loss: 0.0137 - acc: 0.9966 - val_loss: 0.8306 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 8.9228e-04 - acc: 1.000 - ETA: 1s - loss: 0.0026 - acc: 1.0000    - ETA: 1s - loss: 0.0123 - acc: 0.994 - ETA: 1s - loss: 0.0099 - acc: 0.996 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0109 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0100 - acc: 0.996 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - 2s 288us/step - loss: 0.0112 - acc: 0.9969 - val_loss: 0.8529 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0262 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.997 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0085 - acc: 0.998 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0088 - acc: 0.998 - ETA: 0s - loss: 0.0085 - acc: 0.998 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0085 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0088 - acc: 0.998 - ETA: 0s - loss: 0.0088 - acc: 0.998 - ETA: 0s - loss: 0.0087 - acc: 0.998 - ETA: 0s - loss: 0.0085 - acc: 0.998 - ETA: 0s - loss: 0.0087 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.998 - ETA: 0s - loss: 0.0086 - acc: 0.998 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - 2s 295us/step - loss: 0.0086 - acc: 0.9978 - val_loss: 0.8416 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0021 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0016 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.998 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - 2s 289us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.8267 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 9.7671e-04 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 0.9955    - ETA: 1s - loss: 0.0105 - acc: 0.995 - ETA: 1s - loss: 0.0076 - acc: 0.996 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0053 - acc: 0.997 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - 2s 288us/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.8601 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 1.8499e-04 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.0000    - ETA: 1s - loss: 0.0049 - acc: 0.997 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0083 - acc: 0.996 - ETA: 1s - loss: 0.0076 - acc: 0.995 - ETA: 1s - loss: 0.0068 - acc: 0.996 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.996 - ETA: 1s - loss: 0.0086 - acc: 0.995 - ETA: 1s - loss: 0.0084 - acc: 0.996 - ETA: 1s - loss: 0.0077 - acc: 0.996 - ETA: 1s - loss: 0.0079 - acc: 0.996 - ETA: 1s - loss: 0.0075 - acc: 0.996 - ETA: 1s - loss: 0.0079 - acc: 0.996 - ETA: 1s - loss: 0.0083 - acc: 0.996 - ETA: 1s - loss: 0.0078 - acc: 0.996 - ETA: 0s - loss: 0.0075 - acc: 0.996 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.996 - ETA: 0s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0057 - acc: 0.997 - ETA: 0s - loss: 0.0055 - acc: 0.997 - ETA: 0s - loss: 0.0054 - acc: 0.997 - ETA: 0s - loss: 0.0054 - acc: 0.997 - ETA: 0s - loss: 0.0052 - acc: 0.997 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.997 - ETA: 0s - loss: 0.0053 - acc: 0.997 - 2s 288us/step - loss: 0.0053 - acc: 0.9979 - val_loss: 0.8791 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.5150e-04 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.0000    - ETA: 1s - loss: 0.0010 - acc: 1.000 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0187 - acc: 0.996 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.997 - ETA: 1s - loss: 0.0101 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0101 - acc: 0.997 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - 2s 305us/step - loss: 0.0070 - acc: 0.9984 - val_loss: 0.9150 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 1.7811e-05 - acc: 1.000 - ETA: 1s - loss: 3.7830e-04 - acc: 1.000 - ETA: 1s - loss: 3.9444e-04 - acc: 1.000 - ETA: 1s - loss: 4.7234e-04 - acc: 1.000 - ETA: 1s - loss: 4.8487e-04 - acc: 1.000 - ETA: 1s - loss: 7.1744e-04 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.0000    - ETA: 1s - loss: 0.0010 - acc: 1.000 - ETA: 1s - loss: 0.0015 - acc: 0.999 - ETA: 1s - loss: 0.0014 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0046 - acc: 0.999 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - 2s 283us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.9264 - val_acc: 0.8371\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4121d00cc0>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Resnet50_model.fit(train_Resnet50, train_targets, \n",
    "          validation_data=(valid_Resnet50, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "Resnet50_model.load_weights('saved_models/weights.best.Resnet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 79.1866%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - ETA: 8: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 56s - ETA: 44 - ETA: 39 - ETA: 32 - ETA: 28 - ETA: 23 - ETA: 20 - ETA: 17 - ETA: 14 - ETA: 12 - ETA: 10 - ETA: 8 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 4s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Anatolian_shepherd_dog'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "\n",
    "def Resnet50_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = Resnet50_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]\n",
    "\n",
    "Resnet50_predict_breed('dogImages/test/010.Anatolian_shepherd_dog/Anatolian_shepherd_dog_00661.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog breed is Australian_cattle_dog\n"
     ]
    }
   ],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "def classify_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    if len(faces) > 0:\n",
    "        breed=Resnet50_predict_breed(img_path)\n",
    "        return print(\"The image is human and resembling dog breed is {}\".format(breed))\n",
    "    elif dog_detector(img_path) == True:\n",
    "        breed=Resnet50_predict_breed(img_path)\n",
    "        return print(\"The dog breed is {}\".format(breed))\n",
    "    else:\n",
    "        print(\"Error: The image is neither dog nor human\")\n",
    "        \n",
    "classify_image('dogImages/test/011.Australian_cattle_dog/Australian_cattle_dog_00727.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ \n",
    "The algorithm perfectly detected human and dog faces. I am surprised it even differentiated wolf image (the features are nearly same as dog) by not classifing it as dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The image is neither dog nor human\n",
      "The dog breed is Beagle\n",
      "The image is human and resembling dog breed is Chihuahua\n",
      "Error: The image is neither dog nor human\n",
      "Error: The image is neither dog nor human\n",
      "The image is human and resembling dog breed is Beagle\n"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "classify_image('mytest/cat.jpg')\n",
    "classify_image('mytest/dog.jpg')\n",
    "classify_image('mytest/me.jpg')\n",
    "classify_image('mytest/pizza.jpg')\n",
    "classify_image('mytest/wolf.jpg')\n",
    "classify_image('mytest/human.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
